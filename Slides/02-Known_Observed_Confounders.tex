\documentclass{beamer}

\input{preamble.tex}
\usepackage{breqn} % Breaks lines

\usepackage{amsmath}
\usepackage{mathtools}

\usepackage{pdfpages} % \includepdf

\usepackage{listings} % R code
\usepackage{verbatim} % verbatim

% Video stuff
\usepackage{media9}

% packages for bibs and cites
\usepackage{natbib}
\usepackage{har2nat}
\newcommand{\possessivecite}[1]{\citeauthor{#1}'s \citeyearpar{#1}}
\usepackage{breakcites}
\usepackage{alltt}

% tikz
\usepackage{tikz}
\usepackage{pgfplots}
\usetikzlibrary{calc, positioning, decorations.pathreplacing, arrows.meta, intersections}
\pgfdeclarelayer{bg}
\pgfdeclarelayer{back}
\pgfdeclarelayer{fg}
\pgfsetlayers{bg,main,fg,back}
\usetikzlibrary{shapes,arrows}

% Setup math operators
\DeclareMathOperator{\E}{E} \DeclareMathOperator{\tr}{tr} \DeclareMathOperator{\se}{se} \DeclareMathOperator{\I}{I} \DeclareMathOperator{\sign}{sign} \DeclareMathOperator{\supp}{supp} \DeclareMathOperator{\plim}{plim}
\DeclareMathOperator*{\dlim}{\mathnormal{d}\mkern2mu-lim}
\newcommand\independent{\protect\mathpalette{\protect\independenT}{\perp}}
   \def\independenT#1#2{\mathrel{\rlap{$#1#2$}\mkern2mu{#1#2}}}
\newcommand*\colvec[1]{\begin{pmatrix}#1\end{pmatrix}}

\newcommand{\myurlshort}[2]{\href{#1}{\textcolor{gray}{\textsf{#2}}}}


\begin{document}

\imageframe{./lecture_includes/mixtape_ci_cover.png}


% ---- Content ----

\section{Unconfoundedness and Ignorable Treatment Assignment}

\subsection{Choosing Covariates}





\begin{frame}{Adjusting for variables}

\begin{itemize}
\item One of the first things you learn in a methods course is multivariate regression ``controlling for $X$''
\item What is this? Why do we do this?  What should $X$ be? What causal parameter does it help identify?
\item Unconfoundedness, selection on observables, ignorable treatment assignment are different terms describing the same thing -- the RCT is still occurring, only within the dimensions of a conditioning set of confounders and covariates
\end{itemize}

\end{frame}


\begin{frame}{Which covariates?}

\begin{itemize}

\item One of the values of causal directed acyclic graphs (DAG) is it allows you to formally select variables needed for covariate adjustment
\item One such approach is the backdoor criterion which states that if you can condition on $X$ such that all backdoor paths close, then you can identify some aggregate causal parameter
\item But this requires a model, and I don't mean a theoretical model that you might learn as some abstract theory about education
\item It's a model of treatment assignment, which is local in nature, and when it occurs outside the RCT requires expert knowledge

\end{itemize}

\end{frame}




\begin{frame}{Simple DAG}

\begin{figure}
\begin{center}
\caption{A simple DAG illustrating selection on observables.}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$C$};
\node[above left of = d, text centered] (w) {$W$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (c) -- (d);
\draw[->, line width= 1] (w) -- (d);
\draw[->, line width= 1] (c) -- (y);
\draw[->, line width= 1] (w) -- (y);
\end{tikzpicture}
\label{fig:backdoor_dag}
\end{center}
\end{figure}

\bigskip

Write down all paths, both direct from $D$ to $Y$ and indirect or ``backdoor paths'' 

\end{frame}



\begin{frame}{Simple DAG}

\begin{figure}
\begin{center}
\caption{A simple DAG illustrating selection on observables.}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$C$};
\node[above left of = d, text centered] (w) {$W$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (c) -- (d);
\draw[->, line width= 1] (w) -- (d);
\draw[->, line width= 1] (c) -- (y);
\draw[->, line width= 1] (w) -- (y);
\end{tikzpicture}
\end{center}
\end{figure}

\bigskip

\begin{enumerate}
\item[1. ] $D\rightarrow Y$, the direct edge representing a causal effect with associated causal parameter like the ATE, ATT, etc. 
\end{enumerate}
\end{frame}



\begin{frame}{Simple DAG}

\begin{figure}
\begin{center}
\caption{The same simple DAG illustrating selection on observables only with the direct edge from $D$ to $Y$ deleted and backdoor $W$ blocked.}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$C$};
\node[above left of = d, text centered] (w) {$\mybox{W}$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (c) -- (d);
\draw[->, line width= 1] (c) -- (y);
\end{tikzpicture}
\end{center}
\end{figure}

\bigskip

\begin{enumerate}
\item[2. ] $D\leftarrow \mybox{W} \rightarrow Y$ is a backdoor from $D$ to $Y$ through $W$. \textcolor{purple}{Block it}
\end{enumerate}
\end{frame}



\begin{frame}{Remaining variation after blocking}

\begin{figure}
\begin{center}
\caption{Visualization of Backdoor Criterion}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$\mybox{C}$};
\node[above left of = d, text centered] (w) {$\mybox{W}$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\end{tikzpicture}
\end{center}
\end{figure}

\bigskip

\begin{enumerate}
\item[2. ] $D\leftarrow \mybox{W} \rightarrow Y$ is a backdoor from $D$ to $Y$ through $W$. \textcolor{purple}{Block it}
\item[3. ] $D\leftarrow \mybox{C} \rightarrow Y$ is a backdoor from $D$ to $Y$ through $C$. \textcolor{purple}{Block it}
\end{enumerate}
\end{frame}


\begin{frame}{Definition of Known and Quantified Confounders}
	
	
	\begin{block}{Definition of a Known and Quantified Confounder}
	Variable $C$ is a \emph{known} and \emph{quantified} \emph{confounders} if the researcher believes it causes units to select into treatment ($C \rightarrow D$) and also independently determine outcome $Y$, or $C \rightarrow Y$. Confounders are always known, which requires prior knowledge. And to be quantified, they must be correctly measured in your dataset.
	\end{block}
	
	
\end{frame}

\begin{frame}{Known and Quantified Confounder}

	\begin{itemize}
	\item Confounders may or may not be observed, but they must be known if they are confounders as confounders create backdoor paths from $D$ to $Y$
	\item Visually, solid lines means they are ``quantified'' (i.e., in the data), whereas dashed lines mean they are either not defined correctly or not in the dataset (``unobserved'')
	\item Backdoor criterion is appropriate only for known and quantified confounders -- if either known or quantified is missing, this material today is not to be used
	\end{itemize}

\end{frame}

\begin{frame}{DAG tells us what we need to condition on}

\begin{itemize}

\item If we ``block'' on $C$ and $W$, then the \emph{only} explanation of why $D$ and $Y$ are then correlated is causal
\item Depending on the model we estimate, and explicit assumptions made about potential outcomes, then we are able to identify an aggregate causal parameter
\item We call $C$ and $W$ the ``known and quantified confounders'' because the model said these were necessary, they were observed (no dashed line) and they were confounders
\item So what's a collider, and what's a covariate? Let's now add those into the simple DAG

\end{itemize}

\end{frame}



\begin{frame}{Modification of the original DAG}

\begin{figure}
\begin{center}
\caption{A DAG illustrating confounders ($W$ and $C$) versus colliders ($B$) versus exogenous covariates $(X)$.}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$\mybox{C}$};
\node[above left of = d, text centered] (w) {$\mybox{W}$};
\node[above of = y, text centered] (x) {$X$};
\node[below of = y, text centered] (b) {$B$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (d) -- (b);
\draw[->, line width= 1] (y) -- (b);
\draw[->, line width= 1] (x) -- (y);
\end{tikzpicture}
\label{fig:backdoor_dag}
\end{center}
\end{figure}

\begin{enumerate}
\item[4. ] You cannot get from $D$ to $Y$ via $X$ so it is not a backdoor path
\end{enumerate}


\end{frame}




\begin{frame}{Covariate}
	
	
	\begin{block}{Definition of a Covariate}
	Variable $X$ is a covariate if it causes $Y$ but does not cause the treatment status $D$.
	\end{block}
	
	\begin{itemize}
	\item Think of it as in the error term, but not correlated with the treatment variable
	\item Including $X$ in a model can increase precision of estimates of $D$ on $Y$ simply by reducing residual variance, but should have no effect on point estimates
	\item Keep ``confounder'' and ``covariate'' distinct
	\item Covariates can be time invariant or change over the time -- that's not relevant
	\end{itemize}
	
\end{frame}

\begin{frame}{Modification of the original DAG}

\begin{figure}
\begin{center}
\caption{A DAG illustrating confounders ($W$ and $C$) versus colliders ($B$) versus exogenous covariates $(X)$.}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$\mybox{C}$};
\node[above left of = d, text centered] (w) {$\mybox{W}$};
\node[above of = y, text centered] (x) {$X$};
\node[below of = y, text centered] (b) {$B$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (d) -- (b);
\draw[->, line width= 1] (y) -- (b);
\draw[->, line width= 1] (x) -- (y);
\end{tikzpicture}
\label{fig:backdoor_dag}
\end{center}
\end{figure}

\begin{enumerate}
\item[5. ] You cannot get from $D$ to $Y$ via $B$ so it is a collider, but if you control for it, that path opens up and introduces selection bias (``bad controls'')
\end{enumerate}


\end{frame}


\begin{frame}{Colliders}
	
	
	\begin{block}{Definition of a Collider}
	Variable $B$ is a collider if there exists $D \rightarrow B \leftarrow Y$ along the path from $D$ to $Y$. 
	\end{block}
	
	\begin{itemize}
	\item Colliders block backdoor paths so long as they are not blocked
	\item If you block on a collider, then the backdoor path opens, unless there exists a non-collider that you block to close it
	\item Conditioning on a collider introduces selection bias and depending on the magnitudes of $D \rightarrow B$ and $B \leftarrow Y$ relative to $D \rightarrow Y$, the distortion of estimated effect of $D$ on $Y$ may be extreme
	\end{itemize}
	
\end{frame}

\begin{frame}{Summarizing ``which variables''}

\begin{itemize}
\item Comparing treatment and control group of exactly the same values of known and quantified confounders will allow you to estimate aggregate causal parameters of interest
\item Covariates can improve precision but do not reduce bias
\item Colliders must be left alone, otherwise they introduce bias unless another non-collider can block them
\end{itemize}

\end{frame}

\begin{frame}{M-bias as collider}



  \centering
  \includegraphics[scale=0.5,height=6.5cm, width=10cm]{./lecture_includes/mbias}

\footnotesize
$X$ is pre-treatment, but if you conditioned on it, then $D\leftarrow U1 \rightarrow \mbox{X} \leftarrow U2 \rightarrow Y$ and $X$ is a collider. Colliders take strange forms, so the conditioning set \emph{must} be thoughtfully chosen based on being a strong predictor of $Y^0$, preferably based on expert knowledge and not a kitchen sink of all available regressors

\end{frame}



\begin{frame}{Contrast this with ordinary practices}

\begin{itemize}
\item Person attempts to ``control for omitted variable bias'' by including as many ``controls'' as possible
\item Person does not even attempt to think about treatment assignment mechanism and therefore has no idea what variables are colliders, covariates or confounders
\item Big data approaches to covariate adjustment is \emph{very dangerous} -- colliders introduce bias and without a model, there is no way you know what those are
\end{itemize}

\end{frame}

\begin{frame}{Covariate selection without DAGs}

\begin{itemize}
\item Short of an outright DAG, then the thing to be thinking about is this:
	\begin{itemize}
	\item What set of covariates are highly predictive of $Y^0$?
	\item What set of covariates are highly predictive of $D$?
	\item Are these covariates distributed enough across both treatment and control?
	\end{itemize}
\item This is more of a hunch approach, but at least it's based on reasoning through the treatment assignment mechanism as opposed to ``kitchen sink regressions''
\end{itemize}

\end{frame}

\begin{frame}{Falsifications}

\begin{itemize}
\item Covariates should not be affected by the treatment, so examining them as falsifications can help establish the credibility of unconfoundedness
\item Falsification exercises are sometimes versions of this -- a gun control law shouldn't affect automobile theft or petty larceny
\item Imbens and Rubin (2015) suggested using the lagged outcome (pre-treatment) as a way of checking, as those have similar confounder structures
\item One study questioned a finding that obesity was contagious in social networks by estimating the same model on things that cannot be contagious like acne, headaches and height and found the same things (likely confounding existed)
\end{itemize}

\end{frame}





\subsection{Aggregate target parameters}

\begin{frame}{Self selection based on gains}

\begin{itemize}

\item Our focus on variables does not always clarify the exact nature of the matching and regression designs 
\item Covariates are about randomization \emph{within} the dimensions of $X$
\item If actors are randomly assigning themselves to treatments for identical values of $X$, then they are not choosing treatments optimally/rationally
\item Rational actors almost by definition are thought to ``self-select into treatment'' making non-designed comparisons potentially misleading 
\item RCTs can overcome the selection bias that emerges when independence is violated, but are there other ways too?

\end{itemize}

\end{frame}



\begin{frame}{ATE vs ATT}

\begin{itemize}

\item The RCT will identify the ATE, but under randomization, the ATE$=$ATT$=$ATU because randomization of treatment is independent of all potential outcomes
\item But outside the RCT, to identify the ATE requires a stronger set of assumptions than you need for ATT or ATU
\item Questions is which parameter is it you actually do want to know for decision making?  

\end{itemize}

\end{frame}





\begin{frame}{ATE vs ATT}

\begin{itemize}
\item Pfizer wants to vaccinate the world -- since everyone will get the treatment, they want to know the ATE
\item If the goal ultimately is to implement something for every person, usually the ATE is the parameter you want to know (e.g., vaccines)
\item But there is also the ATT -- the returns to the program for those people on the program (e.g., ventilators will only be given to compromised people not all people)
\end{itemize}

\end{frame}


\begin{frame}{Population vs sample analogs of causal parameter}

Define ATE as population mean treatment effects: $$\delta^{ATE}= E[Y_i^1] - E[Y_i^0]$$

Define ATE sample analog as: $$\frac{1}{N} \sum_i^N \delta_i =  \frac{1}{N} \sum_i^N  \bigg [ Y_i^1 - Y_i^0 \bigg ]$$

Where $N$ is the entire sample.  This cannot be measured directly due to missing data on counterfactuals for both the treated and untreated units (``fundamental problem of causal inference'').


\end{frame}

\begin{frame}{Defined causal parameters and weights}

ATE is a weighted average of ATT and ATU:

$${\delta}^{ATE} = \pi \times  {\delta}^{ATT} + (1-\pi) \times {\delta}^{ATU}$$


where $\pi$ is share of population in treatment group, and
\begin{eqnarray*}
{\delta}^{ATT} &=& E[\delta_i | D_i=1] \\
{\delta}^{ATT} &=& E[\delta_i | D_i=0] 
\end{eqnarray*}

\bigskip

Show in an exercise, skip to WEIGHTS tab:

\url{https://docs.google.com/spreadsheets/d/10DuQqGtH_Ewea7zQoLTFYHbnvqaTVDhn2GDzq3Oa6EQ/edit?usp=sharing}

\end{frame}

\begin{frame}{Imputation}

\begin{quote}
``At some level, all methods for causal inference can be viewed as imputation methods, although some more explicitly than others.''
\end{quote}

\bigskip

For whichever parameter, we are always missing counterfactuals, so if we did design a method that estimated that parameter, it must be we imputed the missing conditional mean counterfactual

\bigskip

Some methods do this explicitly -- like synthetic control -- and some it's not clear, but don't ever forget that all of them are doing it

\end{frame}


\begin{frame}[plain]

	\begin{block}{Identifying assumption I: Unconfoundedness}
	$(Y_i^0$, $Y_i^1)$ $\independent{D} | X_i$. There exists a set $X$ of known and quantified confounders such that after adjusting for them, treatment assignment is \emph{independent of potential outcomes}.
	\end{block}
	
	\begin{itemize}
	\item Conditional on $X$, treatment assignment is randomly distributed (i.e., independent of both potential outcomes) -- strong assumption
	\item For a large group of people within the same strata, they flipped coins as opposed to sought treatments that helped them
	\item Eliminating all backdoor paths on a DAG through blocking satisfies unconfoundedness; also called ignorability
	\end{itemize}
\end{frame}


\begin{frame}[plain]

	\begin{block}{Identifying assumption I: Unconfoundedness}
	$(Y_i^0$, $Y_i^1)$ $\independent{D} | X_i$. There exists a set $X$ of known and quantified confounders such that after adjusting for them, treatment assignment is \emph{independent of potential outcomes}.
	\end{block}
	
	\begin{eqnarray*}
	\alert{E[Y^0 | D=1, X=x]} &=& E[Y^0 | D=0, X=x] \\
	E[Y^1 | D=1, X=x] &=& \alert{E[Y^1 | D=0, X=x] }
	\end{eqnarray*}
	
Unconfoundedness justifies substituting units in treatment for control based on $X=x$ -- but only if there are exact matches (next slide)
	
	
\end{frame}



\begin{frame}[plain]

	\begin{block}{Identifying assumption II: Common support}
	For ranges of $X$, there is a positive probability of being both treated and untreated
	\end{block}
	
	\begin{itemize}
	\item There exists units in treatment and control with same values of $X$ -- you can't make the substitutions otherwise
	\item Dimension $k$ means every specific combination of the conditioning set (e.g., not males and old, but adult males, adult females, youth male, youth female)
	\item Testable because common support is observable unlike unconfoundedness, but as you can imagine if the dimensions of $X$ gets large (and with a continuous covariate it's infinite!) then it won't hold in any finite sample!
	\end{itemize}
	
	
\end{frame}



\begin{frame}{Assumptions combined}
	
But if we have them both (represented below), we can even outside of an RCT estimate the ATE through nonparametric matching
  \begin{enumerate}
		\item $(Y^1,Y^0) \independent{D} | X$ (strong unconfoundedness)
		\item $0<Pr(D=1|X)<1$ with probability one (common support)
  \end{enumerate}

\bigskip
Comparing groups of individuals \emph{who have the same values of} $X$, treatment is no longer based gains, $\delta$. 

\bigskip

The second term implies we have people in treatment and control for every strata of $X$
\end{frame}


\begin{frame}{Estimating ATE with Assumptions}


	\begin{itemize}
	\item Unconfoundedness lets you use $Y^0_j$ from control group as $i$'s $\alert{Y^0_i}$ and $Y^1_i$ from treatment group as $j$'s $\alert{Y^1_j}$ using $X$ as the matching guide
		\begin{eqnarray*}
		E[Y^1-Y^0|X] &=& E[Y^1 - Y^0 | X,D=1] \\
		&=&E[Y|X,D=1] - E[Y|X,D=0]
		\end{eqnarray*}
	\item Common support (Assumption 2) allows the match to take place as well as weight over the covariate distribution
		\begin{eqnarray*}
		\delta_{ATE} &=&E[Y^1-Y^0] = E\bigg[ E[Y^1 - Y^0 \ \vert \ X] \bigg] \\
		&=& \int E[Y^1 - Y^0 |X,D=1] dPr(X) \\
		&=& \int \left(E[Y|X,D=1] - E[Y|X,D=0]\right)dPr(X)
		\end{eqnarray*}
	\end{itemize}

\end{frame}



\begin{frame}{Maybe You Want the ATT}

ATE requires conditional independence with respect to both $Y^1$ and $Y^0$ which would mean complete irrationality 

\bigskip

If we want the ATT, we can go with strictly weaker assumptions -- weak unconfoundedness and weak overlap

\bigskip

You can get a PhD because you care about your happiness with one $Y^1$; you just have to have acted independent of what you gave up $Y^0$

\end{frame}




\begin{frame}{ATT Identification}

We can modify those assumptions and weaken both which helps a lot

\begin{enumerate}
  \item $Y^0 \independent{D} | X$ (weak unconfoundedness)
  \item $Pr(D=1|X)<1$ (with $Pr(D=1)>0$) (weak support)
\end{enumerate}

\bigskip

We don't need full common support because we don't need to find counterfactuals for the control group -- we only need units in the control group that match with our treatment group

\bigskip

Selection is weaker too, like I said -- they are not entirely irrational, but who knows if it helps you

\end{frame}



\begin{frame}{Estimating ATT}


Weighted averages under both assumptions:
		\begin{eqnarray*}
		\delta_{ATT} &=& \int \left(E[Y|X,D=1] - E[Y|X,D=0]\right)dPr(X|D=1)
		\end{eqnarray*}
We match units in treatment and control because under weak unconfoundedness they're substitutable, and we use weak common support so that we can actually do it, then we take weighted averages over the differences.  
		
\end{frame}



\begin{frame}{Estimators}

\begin{itemize}

\item Now we will explore estimators that for lack of a better word ``use covariates'' to estimate aggregate causal parameters
\item I will be bundling them around a few topics: exact matching, inexact matching, and regressions
\item Themes about heterogeneous treatment effects, common support and correct (and incorrect) regression specifications will be common

\end{itemize}

\end{frame}





\section{Matching Estimators }


\subsection{Stratification weighting}


\begin{frame}{History of stratification}

\begin{itemize}

	\item Adjusting for confounders was developed within statistics and epidemiology largely to study smoking's effect on lung cancer
	\item Couldn't run RCTs to examine smoking's effect, so people relied on non-experimental data, but results were not plausible to critics for a variety of reasons
	\item Stratification weighting was developed to adjust for the role that known and quantified confounders were playing through

\end{itemize}

\end{frame}






\begin{frame}[plain, shrink=20]
	\begin{figure}
	\includegraphics[scale=0.75]{./lecture_includes/cancer_fig1.pdf}
	\end{figure}

	\begin{figure}
	\includegraphics[scale=0.75]{./lecture_includes/cancer_fig2.pdf}
	\end{figure}

\end{frame}	


\begin{frame}[plain]
	
	\begin{figure}
	\includegraphics[scale=0.5]{./lecture_includes/smoking_figure1.pdf}
	\end{figure}

	\begin{figure}
	\includegraphics[scale=0.5]{./lecture_includes/smoking_figure2.pdf}
	\end{figure}

\end{frame}	
	

	


\begin{frame}{Hypothesis and skepticism}

Early 20th century scientists believed smoking caused lung cancer but others felt the evidence was not strong
		\begin{enumerate}
		\item Sample bias due to non-random selection of subjects (only those who died)
		\item Complaints about functional forms using ``risk ratios'' and ``odds ratios''
		\item Implausible magnitudes (``smell test'')
		\item Killer critique: \emph{no experimental evidence} to incriminate smoking as a cause of lung cancer
		\end{enumerate}
\end{frame}

\begin{frame}{Fisher's genome confounder theory}
		\begin{itemize}
		\item Ronald Fisher (recall from earlier) was a heavy smoker, died of cancer, and paid expert witness for the tobacco industry
		\item Famous as statistician and as a geneticist and using logic, statistics and genetic evidence proposed a contrarian theory that there existed a confounding genome, $Z$, which introduced selection bias into contrasts of smokers and non-smokers 
		\item Studies showed that cigarette smokers and non-smokers were different on observables -- more extraverted than non-smokers, differed in age, differed in income, differed in education, etc.
		\end{itemize}
\end{frame}

\begin{frame}{Fisher's genome confounder theory in DAG form}
	
Smoking, $S$, is only \emph{correlated} with lung cancer, $C$ because of an unknown ant not quantified confounder $Z$:

  \begin{center}
    \begin{tikzpicture}
    [node distance=1.5cm]

    % nodes %
    \node[text centered] (z) {$Z$};
    \node[below right of = z, text centered] (c) {$C$};
    \node[below left of = z, text centered] (s) {$S$};
    
    % edges %
    \draw[->, line width= 0.5] (z) -- (c);
    \draw[->, line width= 0.5] (z) -- (s);
    % \draw[->, line width= 0.5] (s) -- (c);
    
    \end{tikzpicture}
  \end{center}

Legitimate criticism, but ultimately incorrect:

\bigskip

  \begin{quote}  
    ``the [epidemiologists] turned out to be right, but only because bad logic does not necessarily lead to wrong conclusions.'' Robert Hooke (1983)
  \end{quote}

	
\end{frame}

\begin{frame}{Stratification weighting}

\begin{itemize}
\item Simple weighting techniques were eventually introduced to adjust for these observables
\item Stratification weighting goes back at least to Cochran (1968)
\item We will discuss it both using his original paper, and an application
\item Goal is to adjust quantities for a known and quantified confounder through weights based on the confounder's distribution in different samples
\end{itemize}

\end{frame}




\begin{frame}{Mortality rates by country and smoking group}
	
	\begin{table}\centering
		\caption{Death rates per 1,000 person-years (Cochran 1968)}
		\begin{center}
		\begin{tabular}{lccc}
		\hline \hline
		\multicolumn{1}{l}{Smoking group}&
		\multicolumn{1}{c}{Canada}&
		\multicolumn{1}{c}{U.K.}&
		\multicolumn{1}{c}{U.S.}\\
		\hline
		Non-smokers & 20.2 & 11.3 & 13.5\\
		Cigarettes & 20.5 & 14.1 & 13.5 \\
		Cigars/pipes & 35.5 & 20.7 & 17.4\\
		\hline
		\end{tabular}
		\end{center}
	\end{table}
	
	
\end{frame}

\begin{frame}{Mortality rates by country and smoking group}

\begin{itemize}
\item Cigars in these data are particularly odd to me -- much higher mortality rates in all three countries than non-smokers, as well as cigarette smokers. 
\item Another strange result -- cigarette smokers in Canada and US had same mortality rates as non-smokers
\item Seems weird to us, but unclear what they would've thought given the science was unsettled
\item Cochran tackles \emph{one} of the observable variables that he thinks predicts smoking and mortality -- age

\end{itemize}

\end{frame}


\begin{frame}{Non-smokers and smokers differ in age}
	

	\begin{table}\centering
		\caption{Mean ages, years (Cochran 1968)}
		\begin{center}
		\begin{tabular}{lccc}
		\hline \hline
		\multicolumn{1}{l}{Smoking group}&
		\multicolumn{1}{c}{Canada}&
		\multicolumn{1}{c}{U.K.}&
		\multicolumn{1}{c}{U.S.}\\
		\hline
		Non-smokers & 54.9 & 49.1 & 57.0\\
		Cigarettes & 50.5 & 49.8 & 53.2 \\
		Cigars/pipes & 65.9 & 55.7 & 59.7\\
		\hline
		\end{tabular}
		\end{center}
	\end{table}
	
Maybe age is a confounder. It predicts smoking, but it also predicts mortality as cumulative risk of dying grows as we age
 
\end{frame}

\begin{frame}{Imbalanced confounders and covariates}

\begin{itemize}
\item Balance is a phrase often heard in causal inference -- means treatment and group have different covariate/confounder means and/or distributions
\item If confounders are \emph{imbalanced}, then their means differ in each group, and will introduce selection bias (if confounders, but not if covariates)
\item Weighting, matching and regression that adjust for known and quantified confounders attempt to create balance on confounders and covariates so that their effects stop
\end{itemize}

\end{frame}


\begin{frame}{General description of stratification weighting}
	

\begin{enumerate}
\item[1. ]Stratify the confounder by slicing it into ``strata'' (e.g., age is young vs old)
\item[2. ]Compare quantity of interest across each treatment status within each strata
\item[3. ]Create strata specific weights
\item[4. ]Weight quantity of interest by share of units across all strata
\end{enumerate}

\end{frame}



\begin{frame}{Smoking and stratification weighting}

  \begin{enumerate}
		\item Stratify the smoking group into differing age groups or ``bins'' or ``strata'' (e.g., young and old)
		\item Calculate mortality rates separately for young and old and treatment and control (four averages)
		\item Construct ``probability weights'' as the proportion of each smoking group sample within a given age group
		\item For treatment and control group, compute the weighted averages of the age groups mortality rates using the probability weights
  \end{enumerate}
  
  \bigskip
  
This procedure will balance the observed covariate, age, between treatment and control

\end{frame}


\begin{frame}{Smoking and stratification weighting}
	


	\begin{table}\centering
		\begin{center}
		\begin{tabular}{lccc}
		\hline \hline
		\multicolumn{1}{l}{}&
		\multicolumn{1}{c}{Death rates}&
		\multicolumn{2}{c}{Number of}\\
		\multicolumn{1}{l}{}&
		\multicolumn{1}{c}{Pipe-smokers}&
		\multicolumn{1}{c}{Pipe-smokers}&
		\multicolumn{1}{c}{Non-smokers}\\
		\hline
		Age 20-50 & 15 & 11 & 29 \\
		Age 50-70 & 35 & 13 & 9 \\
		Age $+$70 & 50 & 16 & 2 \\
		Total & & 40 & 40 \\		
		\hline
		\end{tabular}
		\end{center}
	\end{table}
	
	\begin{flalign*}
    \only<1-2>{&\text{Question: Calculate average pipe smoker death rate w/out stratification?}&}\\
    \only<2-2>{&15\cdot\left(\frac{11}{40}\right)+35\cdot\left(\frac{13}{40}\right)+50\cdot\left(\frac{16}{40}\right)=35.5} \\
    \only<3-4>{&\text{Counterfactual question: What would the average mortality rate be for }} \\ 
    \only<3-4>{&\text{pipe smokers if they had the same age distribution as the non-smokers?}}\\
    \only<4-4>{&15\cdot\left(\frac{29}{40}\right)+35\cdot\left(\frac{9}{40}\right)+50\cdot\left(\frac{2}{40}\right)=21.2}
	\end{flalign*}

\end{frame}


\begin{frame}[plain]

	\begin{table}\centering
	\caption{Adjusted death rates using 3 age groups (Cochran 1968)}
		\begin{center}
		\begin{tabular}{lccc}
		\hline \hline
		\multicolumn{1}{l}{Smoking group}&
		\multicolumn{1}{c}{Canada}&
		\multicolumn{1}{c}{U.K.}&
		\multicolumn{1}{c}{U.S.}\\
		\hline
		Non-smokers & 20.2 & 11.3 & 13.5 \\
		Cigarettes & 28.3 & 12.8  &  17.7 \\
		Cigars/pipes & 21.2 & 12.0 & 14.2 \\
		\hline
		\end{tabular}
		\end{center}
	\end{table}

\end{frame}

\begin{frame}{Exercise: Titanic sinking}

\begin{itemize}

\item Titanic sank on maiden voyage April 15, 1912 after hitting an iceberg in North Atlantic
\item 2200 on board, but only 700 survived, despite 20 lifeboats with 60 capacity (1200 potential lives could've been saved)
\item Women and children first was a maritime rule to ration lifeboats, but there were different cabins (1st class, 2nd class, etc.) on different levels with different proximity to boats
\item What was the causal effect of 1st class on survival adjusting for $W$ and $C$?
\end{itemize}

\end{frame}


\begin{frame}{Exercise: Titanic DAG}

\begin{figure}
\begin{center}
\caption{Women $W$ and children $C$ first maritime rule is a confounder for estimating first class $D$ effect on surviving $Y$}
\begin{tikzpicture}[node distance=2cm]
% nodes %
\node[text centered] (d) {$D$};
\node[below left of = d, text centered] (c) {$\mybox{C}$};
\node[above left of = d, text centered] (w) {$\mybox{W}$};
\node[right of = d, text centered] (y) {$Y$};
% edges %
\draw[->, line width= 1] (d) -- (y);
\draw[->, line width= 1] (c) -- (d);
\draw[->, line width= 1] (w) -- (d);
\draw[->, line width= 1] (c) -- (y);
\draw[->, line width= 1] (w) -- (y);
\end{tikzpicture}
\label{fig:titanic}
\end{center}
\end{figure}

\bigskip

Backdoor criterion can be satisfied by blocking on $W$ and $C$.  These are our known confounders.  Now we just need data to see if it's quantified.

\end{frame}

\begin{frame}{Titanic exercise}

\begin{enumerate}
\item \textbf{Stratify the confounders}: Our age and sex variables are both binary, so we can only create four strata: male children, female children, male adults, female adults
\item \textbf{Calculate differences within strata}: Calculate average survival rates for each group within each of the four strata and difference within strata
\item \textbf{Calculate probability weights}: Count the number of people in each strata and divide by the total number of souls aboard (crew and passengers)
\item \textbf{Aggregate differences across strata using weights}: Estimate the ATE by aggregating the difference in survival rates over the four strata with each strata-specific difference weighted by that strata's weight
\end{enumerate}

\bigskip

Go to lab

\end{frame}

\begin{frame}{Table 1: Simple counts}

{\renewcommand{\arraystretch}{1.1}
\tabcolsep=1.3\tabcolsep 		
\begin{table}\small\index{rolling!3}
\caption{Differences in female and adult passengers by first class status on the Titanic. }
\centering
\begin{tabular}{lcc|cc}
\toprule
\multicolumn{1}{c}{\textbf{Variable name}}&
\multicolumn{2}{c}{\textbf{First class}}&
\multicolumn{2}{c}{\textbf{All other classes}}\\
\multicolumn{1}{c}{}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}\\
\midrule
Percent adult		&	325 	& 98.2\% & 1,876 & 94.5\% \\
Percent female		& 	325	& 44.6\% & 1,876 & 17.3\% \\
\bottomrule
\end{tabular}
\label{tab:titanic-age}
\end{table}}
\end{frame}


\begin{frame}{Table 2: Stratified sample}

{\renewcommand{\arraystretch}{1.1}
\tabcolsep=1.3\tabcolsep 		
\begin{table}\small\index{rolling!3}
\caption{Counts and Titanic survival rates by strata and first class status.}
\centering
\begin{tabular}{lcc|cc|c}
\toprule
\multicolumn{1}{c}{\textbf{}}&
\multicolumn{2}{c}{\textbf{First class}}&
\multicolumn{2}{c}{\textbf{All other classes}}&
\multicolumn{1}{c}{\textbf{}}\\
\multicolumn{1}{l}{Strata}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}&
\multicolumn{1}{c}{Total}\\
\midrule
Male adult		& 175	& 0.326	& 1,492	& 0.188	& 1,667 \\
Female adult	& 144	& 0.972	& 281	& 0.626	& 425 \\
Male child		& 5		& 1		& 59		& 0.407 	& 64\\
Female child	& 1		& 1		& 44		& 0.613 	& 45\\
\midrule
Total	observations	& 325	&&	1,876	 && 2,201\\
\bottomrule
\end{tabular}
\label{tab:titanic-counts}
\end{table}}


\end{frame}

\begin{frame}{Table 3: Estimates of aggregate parameter}

{\renewcommand{\arraystretch}{1.1}
\tabcolsep=1.3\tabcolsep 		
\begin{table}\tiny\index{rolling!3}
\caption{Differences in survival rates, stratification weights, and estimates of parameters}
\centering
\begin{tabular}{lc|ccc}
\toprule
\multicolumn{1}{l}{\textbf{Strata}}&
\multicolumn{1}{c}{\textbf{Differences in Survival Rates}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATE}$}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATT}$}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATU}$}}\\
\midrule
Male adult		& 0.138 &	0.76 &	0.54	&0.80	\\
Female adult	& 0.346 &	0.19 &	0.44	&0.15	\\
Male child		& 0.593 &	0.03 &	0.02	&0.03	\\
Female child	& 0.387 &	0.02 &	0.00	&0.02	\\
\midrule
\multicolumn{1}{l}{\textbf{}}&
\multicolumn{1}{c}{\textbf{No stratification}}&
\multicolumn{3}{c}{\textbf{Stratification weighted estimates}}\\

 & $\widehat{\textbf{SDO}}$& $\widehat{\textbf{ATE}}$ & $\widehat{\textbf{ATT}}$ & $\widehat{\textbf{ATU}}$ \\
\midrule
\textbf{Estimated coefficient}& 0.35 & 0.20 & 	0.24	 & 0.19   \\
\bottomrule
\end{tabular}
\label{tab:titanic-weights}
\end{table}}

\end{frame}

\begin{frame}{Drop the one female child}

\begin{itemize}
\item We were able to estimate all three causal effect parameters because for all four strata there were units in both treatment and control
\item But if we dropped the only female child in first class from the data, we'd be in trouble bc there wouldn't be any way to calculate a difference for that group
\item But what could we identify?

\end{itemize}

\end{frame}

\begin{frame}

\begin{table}\tiny\index{rolling!3}
\caption{Counts and Titanic survival rates by strata and first class status.}
\centering
\begin{tabular}{lcc|cc|c}
\toprule
\multicolumn{1}{c}{\textbf{}}&
\multicolumn{2}{c}{\textbf{First class}}&
\multicolumn{2}{c}{\textbf{All other classes}}&
\multicolumn{1}{c}{\textbf{}}\\
\multicolumn{1}{l}{Strata}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}&
\multicolumn{1}{c}{Obs}&
\multicolumn{1}{c}{Mean}&
\multicolumn{1}{c}{Total}\\
\midrule
Male adult		& 175	& 0.326	& 1,492	& 0.188	& 1,667 \\
Female adult	& 144	& 0.972	& 281	& 0.626	& 425 \\
Male child		& 5		& 1		& 59		& 0.407 	& 64\\
Female child	& 0		& n/a		& 44		& 0.613 	& 44\\
\midrule
Total	observations	& 324	&&	1,876	 && 2,200\\
\bottomrule
\end{tabular}
\label{tab:titanic-counts2}
\end{table}

\end{frame}

\begin{frame}{ATT is the only one we can get}

\begin{eqnarray}
\widehat{\delta}_{ATT} &=& (0.137 \times 0.54) + (0.346 \times 0.44) + (0.593 \times 0.02)  \nonumber \\
&=& 0.24\text{ or 24 percentage points}
\end{eqnarray}

\end{frame}

\begin{frame}

\begin{table}\tiny\index{rolling!3}
\caption{Differences in survival rates, stratification weights, and estimates of parameters without perfect stratification}
\centering
\begin{threeparttable}
\begin{tabular}{lc|ccc}
\toprule
\multicolumn{1}{l}{\textbf{Strata}}&
\multicolumn{1}{c}{\textbf{Differences in Survival Rates}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATE}$}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATT}$}}&
\multicolumn{1}{c}{\textbf{Weight$_{k,ATU}$}}\\
\midrule
Male adult		& 0.137 	&	0.76 &	0.54	&0.80	\\
Female adult	& 0.346 	&	0.19 &	0.44	&0.15	\\
Male child		& 0.593 	&	0.03 &	0.02	&0.03	\\
Female child	& n/a 	&	n/a 	&	n/a	&0.02	\\
\midrule
\multicolumn{1}{l}{\textbf{}}&
\multicolumn{1}{c}{\textbf{No stratification}}&
\multicolumn{3}{c}{\textbf{Stratification weighted estimates}}\\

 & $\widehat{\textbf{SDO}}$& $\widehat{\textbf{ATE}}$ & $\widehat{\textbf{ATT}}$ & $\widehat{\textbf{ATU}}$ \\
\midrule
\textbf{Estimated coefficient}& 0.35 & n/a & 	0.24	 & n/a  \\
\bottomrule
\end{tabular}
\begin{tablenotes}
\tiny
\item 
Differences in survival rates, stratification weights, and estimated parameters. All coefficients should be multiplied by 100 to get a percentage point change in survival rate as a result of having a first class cabin. Note that the SDO is a simple difference in mean outcomes and therefore \emph{not} a weighted average over the strata differences.  But the estimated ATE, ATT and ATU parameters are weighted averages in difference in means using corresponding stratification weights. 
\end{tablenotes}
\end{threeparttable}
\label{tab:titanic-weights2}
\end{table}


\end{frame}

\begin{frame}{Why did this happen?}

\begin{itemize}

\item Stratification requires having units in both groups for every value of $X$ to get ATE
\item If you want the ATT, you have to have units in the control group for every treated group based on its value of $X$ (female children weren't treated, so didn't matter)
\item If you want the ATU, you have to have units in the treatment group for every treated group based on its value of $X$ (female children weren't treated, so did matter)
\item This has a technical word we are going to learn more about called a ``lack of common support''
\end{itemize}

\end{frame}

\subsection{Conditional Independence}

\begin{frame}{Potential outcomes}

\begin{itemize}
\item Now let's introduce potential outcomes notation so that we can build more formalized models
\item This helps us understand what assumptions we need to make, which is not necessarily spelled out in the DAG backdoor criterion
\end{itemize}

\end{frame}

\begin{frame}{Recall the RCT Assumption of Independence}
	
	\begin{itemize}
	\item Randomized treatment assignment guarantees ``independence'' $$(Y^0,Y^1)\independent{D}$$
	\item Independence allows to estimate accurate causal effects through simple methods like differences in averages
		\begin{eqnarray*}
		E[Y|D=1] - E[Y|D=0] &=& \underbrace{E[Y^1|D=1] - E[Y^0|D=0]}_{\mathclap{\text{by the switching equation}}} \\
		&=& \underbrace{E[Y^1] - E[Y^0]}_{\mathclap{\text{by independence}}} \\
		&=& \underbrace{E[Y^1-Y^0]}_{\mathclap{\text{ATE}}}
		\end{eqnarray*}
	\end{itemize}
\end{frame}

\begin{frame}{Confounder and covariate distribution}

\begin{itemize}
\item Just like independence implies balance on expected potential outcomes, it also implies balance on confounders and covariates which is called ``common support''
\item We saw this in our Thornton regressions: cash vouchers were not associated with being male, one's age, etc.
\item If we have balance on potential outcomes, that's all we need but as that's not observed, balance on covariates is often used to provide some evidence the randomization was done well
\end{itemize}
\end{frame}

\begin{frame}{Violations of independence}

\begin{itemize}
\item Problem with smoking and cancer was smoking wasn't \emph{randomly assigned} -- it was chosen by the ``perfect doctor'' (i.e., self selection into smoking based on factors related to potential outcomes)
\item When a treatment is ``dependent'' on potential outcomes, it means people smoke because they expect something is better when they smoke ($Y^1$) than when they don't ($Y^0$) introducing selection bias and potentially heterogenous treatment effect bias
\item Naive comparisons can be deeply misleading -- covariate adjustment can resolve this if ``conditional independence'' happens in the data
\item How do we express backdoor criterion using potential outcomes notation?
\end{itemize}

\end{frame}



\begin{frame}[plain]

	\begin{block}{Identifying assumption I: Conditional independence}
	$(Y_i^0$, $Y_i^1)$ $\independent{D} | X_i$. There exists a set $X$ of known and quantified confounders such that after adjusting for them, treatment assignment is \emph{independent of potential outcomes}.
	\end{block}
	
	\begin{itemize}
	\item Conditional on $X$, treatment assignment is \textbf{random}
	\item For a large group of people within the same strata, they flipped coins as opposed to sought treatments that helped them
	\item Sometimes this is called also ``unconfoundedness'' and worth thinking long and hard about
	\end{itemize}
\end{frame}


\begin{frame}[plain]

	\begin{block}{Identifying assumption I: Conditional independence}
	$(Y_i^0$, $Y_i^1)$ $\independent{D} | X_i$. There exists a set $X$ of known and quantified confounders such that after adjusting for them, treatment assignment is \emph{independent of potential outcomes}.
	\end{block}
	
	\begin{eqnarray*}
	E[Y^0 | D=1, X=x] &=& E[Y^0 | D=0, X=x] \\
	E[Y^1 | D=1, X=x] &=& E[Y^1 | D=0, X=x] 
	\end{eqnarray*}
	
	Allows us to write down these equalities, which means we can use comparison units as substitutes for counterfactuals so long as there exist one-to-one within $X=x$, leading to our next assumption -- common support
	
\end{frame}



\begin{frame}[plain]

	\begin{block}{Identifying assumption II: Common support}
	For ranges of $X$, there is a positive probability of being both treated and untreated
	\end{block}
	
	\begin{itemize}
	\item There exists units in treatment and control with same values of $X$
	\item Dimension $k$ means every specific combination of the conditioning set (e.g., not males and old, but adult males, adult females, youth male, youth female)
	\item Testable, and often this is where regression steps don't incorporate it
	\end{itemize}
	
	
\end{frame}



\begin{frame}{Caveat: Perfect Doctor and conditional independence}

\begin{itemize}
\item Independence was violated if the treatment was assigned \emph{because} we expected things to improve or not (``perfect doctor'' reasoning)
\item If you take an action because you think it helps and others don't take the action because they don't or can't, then it is a violation of independence probably
\item For a large group of people within the same strata, they flipped coins as opposed to sought treatments that helped them
\item Rationality is contained in the confounders -- worth reflecting a lot on
\end{itemize}

\end{frame}


\begin{frame}{Assumptions combined}
	
We need only two assumptions to estimate the ATE, but these are not trivial: 
  \begin{enumerate}
		\item $(Y^1,Y^0) \independent{D} | X$ (conditional independence)
		\item $0<Pr(D=1|X)<1$ with probability one (common support)
  \end{enumerate}

\bigskip
Comparing groups of individuals \emph{who have the same values of} $X$, treatment is no longer based gains, $\delta$. 

\bigskip

The second term implies we have people in treatment and control for every strata of $X$
\end{frame}


\begin{frame}{Implications of assumptions}


	\begin{itemize}
	\item Assumption 1 lets you plug $Y$ for $Y^j$ with the switching equation
		\begin{eqnarray*}
		E[Y^1-Y^0|X] &=& E[Y^1 - Y^0 | X,D=1] \\
		&=&E[Y|X,D=1] - E[Y|X,D=0]
		\end{eqnarray*}
	\item Assumption 2 lets you weight over the covariate distribution
		\begin{eqnarray*}
		\delta_{ATE} &=&E[Y^1-Y^0] = E\bigg[ E[Y^1 - Y^0 \ \vert \ X] \bigg] \\
		&=& \int E[Y^1 - Y^0 |X,D=1] dPr(X) \\
		&=& \int \left(E[Y|X,D=1] - E[Y|X,D=0]\right)dPr(X)
		\end{eqnarray*}
	\end{itemize}

\end{frame}




\begin{frame}{You need fewer assumptions for ATT or ATU}

Other versions of conditional independence

\begin{enumerate}
  \item $Y^0 \independent{D} | X$ 
  \item $Pr(D=1|X)<1$ (with $Pr(D=1)>0$)
\end{enumerate}

\bigskip
Notice how there is only one potential outcome in the independence equation.  That's okay.  We can still then estimate the ATT (just not the ATE).

\end{frame}



\begin{frame}{Summarizing}


Weighted averages under either assumption:
		\begin{eqnarray*}
		\delta_{ATE} &=& \int \left(E[Y|X,D=1] - E[Y|X,D=0]\right)dPr(X) \\
		\delta_{ATT} &=& \int \left(E[Y|X,D=1] - E[Y|X,D=0]\right)dPr(X|D=1)
		\end{eqnarray*}
ATE needs independence with respect to both potential outcomes; ATT only needs it with respect to $Y^0$. 		
		
\end{frame}

\begin{frame}{Review stratification weighting in light of this}

\begin{itemize}

\item Let's now look at the way in this works within stratification if we are missing some observations
\item The reason that this happens is because the dimensions $K$ of the $X$ conditioning set is getting larger

\end{itemize}

\end{frame}

\begin{frame}{Weighting by Age ($K=2$)}
	
	\begin{table}\centering
		\begin{center}
		\begin{tabular}{lccccc}
		\hline \hline
		\multicolumn{1}{l}{}&
		\multicolumn{2}{c}{Death Rate}&
		\multicolumn{1}{c}{}&
		\multicolumn{2}{c}{Number of}\\

		\multicolumn{1}{l}{$X_k$}&
		\multicolumn{1}{c}{Smokers}&
		\multicolumn{1}{c}{Non-smokers}&
		\multicolumn{1}{c}{Diff.}&
		\multicolumn{1}{c}{Smokers}&
		\multicolumn{1}{c}{Non-smokers}\\
		\hline
		Old & 28 & 24 & 4 & 3 & 10 \\
		Young & 22 & 16 & 6 & 7 & 10 \\
		Total & & & & 10 & 20 \\
		\hline
		\end{tabular}
		\end{center}
	\end{table}
	
	

	\begin{flalign*}
		    \only<1-2>{&\text{Question: What is $\widehat{\delta_{ATE}}=\sum_{k=1}^K(\overline{Y}^{1,k} - \overline{Y}^{0,k})\cdot \left(\frac{N^k}{N}\right)$?}&}\\
		    \only<2-2>{&4\cdot\left(\frac{13}{30}\right)+6\cdot\left(\frac{17}{30}\right)=5.13} \\
		    \only<3-4>{&\text{Question: What is $\widehat{\delta_{ATT}}=\sum_{k=1}^K(\overline{Y}^{1,k} - \overline{Y}^{0,k})\cdot \left(\frac{N^k_T}{N_T}\right)$?}} \\ 
		    \only<4-4>{&4\cdot\left(\frac{3}{10}\right)+6\cdot\left(\frac{7}{10}\right)=5.4} 
	\end{flalign*}

\end{frame}
	
		
 

\begin{frame}[shrink=20,plain]{Weighting by Age and Gender ($K=4$)}
	
	\begin{table}\centering
		\begin{center}
		\begin{tabular}{lccccc}
		\hline \hline
		\multicolumn{1}{l}{}&
		\multicolumn{2}{c}{Death Rate}&
		\multicolumn{1}{c}{}&
		\multicolumn{2}{c}{Number of}\\

		\multicolumn{1}{l}{$X_k$}&
		\multicolumn{1}{c}{Smokers}&
		\multicolumn{1}{c}{Non-smokers}&
		\multicolumn{1}{c}{Diff.}&
		\multicolumn{1}{c}{Smokers}&
		\multicolumn{1}{c}{Non-smokers}\\
		\hline
		Old Males & 28 & 22 & 4 & 3 & 7 \\
		Old Females &   & 24 &   &   & 3 \\
		Young Males & 21 & 16 & 5 & 3 & 4 \\
		Young Females & 23 & 17 & 6 & 4 & 6 \\
		Total & & & & 10 & 20 \\
		\hline
		\end{tabular}
		\end{center}
	\end{table}
	
	

	\begin{flalign*}
		    \only<1-2>{&\text{Problem: What is $\widehat{\delta_{ATE}}=\sum_{k=1}^K(\overline{Y}^{1,k} - \overline{Y}^{0,k})\cdot \left(\frac{N^k}{N}\right)$?}&}\\
		    \only<2-2>{&\text{Not identified! What went wrong?}} \\
		    \only<3-4>{&\text{Question: What is $\widehat{\delta_{ATT}}=\sum_{k=1}^K(\overline{Y}^{1,k} - \overline{Y}^{0,k})\cdot \left(\frac{N^k_T}{N_T}\right)$?}} \\ 
		    \only<4-4>{&4\cdot\left(\frac{3}{10}\right)+5\cdot\left(\frac{3}{10}\right)+6\cdot\left(\frac{4}{10}\right)=5.1}
	\end{flalign*}

\end{frame}

\begin{frame}{Curse of Dimensionality}
	
	\begin{itemize}
	\item Stratification methods, including OLS, may become less feasible in finite samples as the number of covariates grows (e.g., $K=4$ was too many for this sample)
	\item Assume we have $k$ covariates and we divide each into 3 coarse categories (e.g., age: young, middle age, old; income: low, medium, high, etc.)
	\item The number of strata is $3^k$. For $k=10$, then it's $3^{10}=59,049$
	\item The problem isn't just the number of covariates; it's the number of strata based on those covariates (you can hit the curse fast)
	\end{itemize}
\end{frame}

\begin{frame}{Curse of Dimensionality}
	
	\begin{itemize}
	\item If sparseness occurs, it means many cells may contain either only treatment units or only control units but not both, and that violates our second assumption
	\item We can always use ``finer'' classifications, but finer cells worsens the dimensional problem, so we don't gain  much from that.  ex: using $10$ variables and $5$ categories for each, we get $5^{10} = 9,765,625$.  
	\item Matching methods really force us to see these curses; they're often hidden from OLS because OLS doesn't tell us it is just doing various extrapolations
	\item Simple weighting methods is also a problem if the cells are ``too coarse''
	\end{itemize}
\end{frame}	


\subsection{Exact and Inexact Matching}

\begin{frame}{Exact matching}

\begin{figure}[!t]\centering
\includegraphics[scale=1.1]{./lecture_includes/identical_twins}
\end{figure}

\end{frame}

\begin{frame}{Exact Matching}


Matching goes back at least to Rubin's early work on the propensity score, but we will start with nearest neighbor matching as there's ideas there we draw upon later with synth I want to emphasize

\bigskip

Matching will match a treated unit to a comparison unit that is identical on the known and quantified confounders

\bigskip

If we can't find one, it means common support failed, the estimate would need to use nearest neighbors (with matching bias), which we will discuss



\end{frame}



\begin{frame}{Training example (unmatched)}

\begin{table}[htb]\tiny\index{rolling!3}
\centering
\tiny
\begin{tabular}{ccc|ccc}
\toprule
	\multicolumn{3}{c}{\textbf{Trainees}}&
	\multicolumn{3}{c}{\textbf{Non-Trainees}}\\
	\multicolumn{1}{c}{Unit}&
	\multicolumn{1}{c}{Age}&
	\multicolumn{1}{c}{Earnings}&
	\multicolumn{1}{c}{Unit}&
	\multicolumn{1}{c}{Age}&
	\multicolumn{1}{c}{Earnings}\\
\midrule
1 & 	31 & \$	26,629 & 	1 & 	29 & \$	23,178 \\
2 & 	31 & \$	26,633 & 	2 & 	39 &\$ 	33,817 \\
3 & 	18 & 	\$	15,324 & 	3 & 	33 & \$	27,061 \\
4 & 	32 &\$ 	27,717 & 	4 & 	46 & \$	43,109 \\
5 & 	32 & \$	27,725 & 	5 & 	32 & \$	26,040 \\
6 & 	25 & \$	20,762 & 	6 & 	39 & \$	33,815 \\
7 & 	32 &\$ 	27,716 & 	7 & 	31 & \$	25,052 \\
8 & 	32 & \$	27,719 & 	8 & 	33 & \$	27,060 \\
9 & 	20 & \$	16,723 & 	9 & 	25 &\$ 	19,787 \\
10 & 	29 & \$	24,552 & 	10 & 	29 &\$ 	23,173 \\
&&&			11 & 	27 & 	21,416 \\
&&&			12 & 	32 & 	26,040 \\
&&&			13 & 	20 & 	16,246 \\
&&&			14 & 	41 & 	36,316 \\
&&&			15 & 	18 & 	15,046 \\
&&&			16 & 	29 & 	23,178 \\
&&&			17 & 	49 & 	47,559 \\
&&&			18 & 	32 & 	26,040 \\
&&&			19 & 	27 & 	21,418 \\
&&&			20 & 	46 & 	43,109 \\
\midrule
Mean & 	28.2 & 	\$24,150 & 	Mean & 	32.85 & 	\$27,923 \\
\bottomrule
\end{tabular}
\end{table}
\tiny
$SDO = \$24,150-27,923 = -\$3,773$


\end{frame}


\begin{frame}{Age Imbalance}

\begin{figure}[!t]\centering
\caption{Age distribution of a job training program's trainees (figure a) versus a sample of workers who were not enrolled in the trainee program (figure b).}
\includegraphics[scale=0.075]{./lecture_includes/trainees.jpg}
\end{figure}

\end{frame}

\begin{frame}{Age Imbalance}

\begin{figure}[!t]\centering
\includegraphics[scale=0.075]{./lecture_includes/nontrainees}
\end{figure}

\end{frame}

\begin{frame}{Exact matching}

\begin{itemize}
\item Exact matching finds a person in the control group whose value of $X_j$ is \emph{exactly} equal to each person in the treatment group $i$
\item Will not work if the conditioning set includes a continuous variable
\item Will also not work if $K$ gets large (curse of dimensionality we discuss later)
\end{itemize}

\end{frame}



\begin{frame}{ATT estimator}

We will focus on the ATT for the rest of today and the equation is:

\begin{equation}
\widehat{\delta}_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1}(Y_i - Y_{j(i)})
\label{eq:att_simplematch}
\end{equation}

where $Y_{j(i)}$ is the $j$\textsuperscript{th} unit matched to the $i$\textsuperscript{th} unit based on the $j$\textsuperscript{th} being ``exactly equal to'' the $i$\textsuperscript{th} unit with respect to the $X$ conditioning set

\end{frame}

\begin{frame}{Number of matches}

What if I find two or more $M$ units with the identical $X$ value? Then what?

\begin{equation}
\widehat{\delta}_{ATT} = \dfrac{1}{N_T} \sum_{D_i=1} \bigg ( Y_i - \bigg [\dfrac{1}{M} \sum_{m=1}^M Y_{j_m(1)} \bigg ] \bigg )
\label{eq:att_match}
\end{equation}

\bigskip

Notice that we are only dealing with $Y^0_i$ by matching; The $Y^1_i$ is fine as is.

\end{frame}

\begin{frame}{Matching algorithm}

\begin{enumerate}
\item For each unit $i$ in the treatment group with known and quantified confounder $X=x_i$, find all units $j$ in the donor pool for whom $x_i=x_j$. These $j$ units are our $M$ matches and $M$ can be one or it can be greater than one if you want it to be.
\item For each unit $i$, replace its missing potential outcome, $Y^0_i$, with the matched $j$ units' realized outcomes, $\frac{1}{M} \sum {Y}_{j(i)}$, from Step 1. Do this for all $i$ units in the treatment group.
\item For each unit $i$, calculate the difference between realized earnings and matched earnings, $\widehat{\delta}_i=Y_i - \frac{1}{M} \sum {Y}_{j(i)}$.
\item Finally, estimate the sample ATT by averaging over all $i$ differences in earnings from Step 3 as $\frac{1}{N_T} \sum \widehat{\delta}_i$, where $N_T$ is the number of treatment units.
\end{enumerate}

\end{frame}



\begin{frame}{Matched sample}

\begin{table}[htb]\small\index{rolling!3}
\caption{Training example with matched sample using exact matching}
\centering
\begin{tabular}{ccc|ccc}
\toprule
	\multicolumn{3}{c}{\textbf{Trainees}}&
	\multicolumn{3}{c}{\textbf{Matched Sample}}\\
	\multicolumn{1}{c}{Unit}&
	\multicolumn{1}{c}{Age}&
	\multicolumn{1}{c}{Earnings}&
	\multicolumn{1}{c}{Matched Unit}&
	\multicolumn{1}{c}{Age}&
	\multicolumn{1}{c}{Earnings}\\
\midrule
1&	31&	\$26,693&	2&	31&	\$25,052 \\
2&	31&	\$26,691&	2&	31&	\$25,052 \\
3&	18&	\$15,392&	18&	18&	\$15,046 \\
4&	32&	\$27,776&	5&	32&	\$26,045 \\
5&	32&	\$27,779&	5&	32&	\$26,045 \\
6&	25&	\$20,821&	4&	25&	\$19,787 \\
7&	32&	\$27,778&	5&	32&	\$26,045 \\
8&	32&	\$27,780&	5&	32&	\$26,045 \\
9&	20&	\$16,781&	8&	20&	\$16,246 \\
10&	29&	\$24,610&	6&	29&	\$23,178 \\
\midrule
Mean&	28.2&	\$24,210&	Mean&	28.2&	\$22,854 \\
\bottomrule
\end{tabular}
\end{table}
\footnotesize
$\widehat{ATT} = \$24,210 - \$22,854 = \$1,356$


\end{frame}

\begin{frame}{Estimated ATT using Exact Matching}


Weak unconfoundedness of $Y^0$ with respect to age justified substituting one group for another

\bigskip

But matching bias still exists if you fail common support -- unconfoundedness is necessary but not sufficient

\bigskip

Even weak support is rare due to the curse of dimensionality

\end{frame}





\begin{frame}{Inexact matching}

\begin{figure}[!t]\centering
\includegraphics[scale=0.75]{./lecture_includes/fraternal_twins}
\end{figure}

\end{frame}


\begin{frame}{Curse of Dimensionality}
	
	\begin{itemize}
	\item If no matches can be found, it means many cells may contain either only treatment units or only control units but not both, and that violates our common support assumption
	\item We can always use ``finer'' classifications, but finer cells worsens the dimensional problem, so we don't gain  much from that.  ex: using $10$ variables and $5$ categories for each, we get $5^{10} = 9,765,625$.  
	\item Matching methods really force us to see these curses; they're often hidden from OLS because OLS uses extrapolations based off functional form
	\end{itemize}
\end{frame}	


\begin{frame}{Propensity scores}

\begin{itemize}
\item Propensity scores were developed by Rosenbaum and Rubin (1983) as a way of reducing the dimension of the conditioning set of $X$
\item But it still requires common support -- propensity scores don't solve the curse of dimensionality problem from the perspective of bias
\item If you don't have exact matches in the dimensions of $X$ then you'll still be matching units with similar propensity scores but it won't overcome the bias
\item There are methods that will adjust the propensity score estimation which I'll briefly mention at the end

\end{itemize}

\end{frame}




\begin{frame}{To Look Like Someone Else}

\begin{itemize}
\item When we can make synthetic xerox copies of ourselves, that's exact matching
\item But what if we can only make similar copies of ourselves, like fraternal, but not identical, twins? That's nearest neighbor matching -- a form of ``inexact matching'', sort of like fraternal twins
\item Introduces bias bc of inexact matching, but the magnitude of the bias depends on the severity of the discrepancy
\item We can improve on nearest neighbor matching using bias adjustment (Abadie and Imbens 2011) 
\end{itemize}

\end{frame}

\begin{frame}{Nearest Neighbor Matching}
	
	\begin{itemize}
	\item Estimate $\widehat{\delta}_{ATT}$ by \emph{imputing} the missing potential outcome of each treatment unit $i$ using the observed outcome from that outcome's ``nearest'' neighbor $j$ in the control set using $X$ for the matching
		\begin{eqnarray*}
		\widehat{\delta}_{ATT} = \frac{1}{N_T}\sum_{D_i=1} (Y_i - Y_{j(i)})
		\end{eqnarray*}where $Y_{j(i)}$ is the observed outcome of a control unit such that $X_{j(i)}$ is the \textbf{closest} value to $X_i$ among all of the control observations (eg match on $X$)
	\end{itemize}
\end{frame}

\begin{frame}{Matching}
	
	\begin{itemize}
	\item We could also use the average observed outcome over $M$ closest matches:
		\begin{eqnarray*}
		\widehat{\delta}_{ATT} = \frac{1}{N_T}\sum_{D_i=1}\left(Y_i-\left[\frac{1}{M}\sum_{m=1}^MY_{j_m(1)}\right]\right)
		\end{eqnarray*}
	\item Works well when we can find good matches for each treatment group unit, so $M$ is usually defined to be small (i.e., $M=1$ or $M=2$)
	\end{itemize}
\end{frame}



\begin{frame}{Matching example with single covariate}
	
	\begin{table}
	\begin{tabular}{c|c|c|c|c}
	\hline
	$i$ & $Y^1_i$ & $Y^0_i$ & $D_i$ & $X_i$ \\
	\hline
	1 & 6 &  \textcolor{red}{?} & 1 & 3 \\
	2 & 1 &  \textcolor{red}{?} & 1 & 1 \\
	3 & 0 &   \textcolor{red}{?} & 1 & 10 \\
	\hline
	4 &  & 0 & 0 & 2 \\
	5 &  & 9 & 0 & 3 \\
	6 &  & 1 & 0 & -2 \\
	7 &  & 1 & 0 & -4 \\
	\hline
	\end{tabular}
	\end{table}
	
	
	\begin{flalign*}
		    \only<1-2>{&\text{Question: What is }\widehat{\delta}_{ATT}=\frac{1}{N_T} \sum_{D_i=1} (Y_i - Y_{j(i)})\text{?}}& \\
		    \only<2-2>{&\text{Match and plug in!}} \\
 	\end{flalign*}

\end{frame}
	
	
\begin{frame}{Matching example with single covariate}
	
	\begin{table}
	\begin{tabular}{c|c|c|c|c}
	\hline
	$i$ & $Y^1_i$ & $Y^0_i$ & $D_I$ & $X_i$ \\
	\hline
	1 & 6 &  \textcolor{blue}{9} & 1 & \textcolor{blue}{3} \\
	2 & 1 &  \textcolor{green}{0} & 1 & \textcolor{green}{1} \\
	3 & 0 &   \textcolor{blue}{9} & 1 & \textcolor{blue}{10} \\
	\hline
	4 &  & \textcolor{green}{0} & 0 & \textcolor{green}{2} \\
	5 &  & \textcolor{blue}{9} & 0 & \textcolor{blue}{3} \\
	6 &  & 1 & 0 & -2 \\
	7 &  & 1 & 0 & -4 \\
	\hline
	\end{tabular}
	\end{table}
	
	
	\begin{flalign*}
		&\text{Question: What is }\widehat{\delta_{ATT}}=\frac{1}{N_T} \sum_{D_i=1} (Y_i - Y_{j(i)})\text{?}& \\
		&\widehat{\delta}_{ATT} = \frac{1}{3} \cdot (6-\textcolor{blue}{9}) + \frac{1}{3} \cdot (1-\textcolor{green}{0}) + \frac{1}{3} \cdot (0-\textcolor{blue}{9}) = -3.7
	\end{flalign*}

\end{frame}


%{
%\setbeamercolor{background canvas}{bg=}
%\includepdf[pages=1-last]{./lecture_includes/abadie-matching2.pdf}
%}



\begin{frame}{Measuring the matching discrepancy}

\begin{itemize}
\item What does it mean to be close when I am working with a large number of covariates?
\item What if we had a way of measuring a match in terms of how ``close'' each unit's $X_i$ value was to the matched $X_j$
\item Let's do that and use the square root of the sum of all squared differences in each unit's $X_i - X_{j(i)}$ as a measure of how bad the match is
\item This is called the Euclidean distance
\end{itemize}


\end{frame}

%\begin{frame}{Alternative distance metric: Euclidean distance}
	
% When the vector of matching covariates, $X= \colvec{X_1\\X_2\\\vdots\\X_k}$ has more than one dimension ($k>1$) we will need a new definition of \textbf{distance} to measure ``closeness''.  
%\end{frame}

\begin{frame}{Euclidean distance}
	\begin{block}{Definition: Euclidean distance}
    \vspace*{-2.5mm}
    \begin{eqnarray*}
    ||X_i-X_j|| &=& \sqrt{ (X_i-X_j)'(X_i-X_j) } \\
    &=& \sqrt{ \sum_{n=1}^k (X_{ni} - X_{nj})^2 }
    \end{eqnarray*}
    \vspace*{-2.5mm}
	\end{block}
	
Let's do this together -- sometimes it helps to manually calculate this

\bigskip

\url{https://docs.google.com/spreadsheets/d/1iro1Qzrr1eLDY_LJVzOYvnQZWmxY8JyTcDf6YcdhkwQ/edit?usp=sharing}

	
\end{frame}


\begin{frame}{Inexact matching: Random match 1}

\begin{figure}[!t]\centering
\includegraphics[scale=0.45]{./lecture_includes/inexact_random1}
\end{figure}

\end{frame}


\begin{frame}{Inexact matching: Random match 2}

\begin{figure}[!t]\centering
\includegraphics[scale=0.45]{./lecture_includes/inexact_random2}
\end{figure}

\end{frame}


\begin{frame}{Minimizing the Euclidean distance}

\begin{itemize}
\item Abadie and Imbens (2006) show that there exists a unique solution to the matching problem that minimizes a given distance metric 
\item \texttt{Matching} in R and \texttt{teffects} in Stata (not sure in python)
\item But the idea here is that any other match will always have a higher Euclidean distance so I've drawn a picture!
\end{itemize}

\end{frame}


\begin{frame}{Visualization of Optimal Match}

\begin{figure}[!t]\centering
\includegraphics[scale=0.35]{./lecture_includes/optimal_matches}
\end{figure}

\end{frame}


 

\begin{frame}{Inexact matching by minimizing the Euclidean distance}

\begin{figure}[!t]\centering
\includegraphics[scale=0.45]{./lecture_includes/inexact_euclidean}
\end{figure}

\end{frame}

\begin{frame}{Other distance metrics}

\begin{itemize}
\item Our example treated a one unit difference in age and one unit difference in GPA as the same, but those scales are different and matter a lot
\item The Euclidean distance is not invariant to changes in the scale of the $X$'s.  
\item Alternative distance metrics that \emph{are} invariant to changes in scale are more commonly used 
\item Normalized Euclidean distance and Mahalanobis distance both try to normalize it so that scale doesn't matter
\end{itemize}

\end{frame}


\begin{frame}{Normalized Euclidean distance}

	\begin{block}{Definition: Normalized Euclidean distance}
	  A commonly used distance is the normalized Euclidean distance:$$||X_i-X_j|| = \sqrt{ (X_i-X_j)'\widehat{V}^{-1}(X_i - X_j) }$$ where
		$$\widehat{V}^{-1} = \text{diag}(\widehat{\sigma}_1^2, \widehat{\sigma}_2^2, \dots, \widehat{\sigma}_k^2)$$
	\end{block}
\end{frame}

\begin{frame}{Normalized Euclidean distance}
	\begin{itemize}
	\item Notice that the normalized Euclidean distance is equal to:
		\begin{eqnarray*}
		||X_i - X_j|| = \sqrt{\sum_{n=1}^k \frac{(X_{ni} - X_{nj})^2}{\widehat{\sigma}^2_n}}
		\end{eqnarray*}
	\item Thus, if there are changes in the scale of $X_{ni}$, these changes also affect $\widehat{\sigma}^2_n$, and the normalized Euclidean distance does not change
	\end{itemize}

\end{frame}


\begin{frame}{Mahalanobis distance}
	
	\begin{block}{Definition: Mahalanobis distance}
	The Mahalanobis distance is the scale-invariant distance metric:
		\begin{eqnarray*}
		||X_i-X_j|| = \sqrt{ (X_i-X_j)'\widehat{\Sigma}_X^{-1}(X_i - X_j) }
		\end{eqnarray*}
	where $\widehat{\Sigma}_X$ is the sample variance-covariance matrix of $X$.
	\end{block}


\end{frame}



\begin{frame}{Matching and the Curse of Dimensionality}
	
\begin{itemize}
\item The larger the dimensions of the conditioning set, the less likely common support holds, and you can't not do it because you need these covariate dimensions to satisfy weak unconfoundedness!
\item This problem is caused by the finite dataset, and it introduces a particular type of selection bias
\item Curses are only overcome with new spells  
\item Abadie and Imbens (2011) derived a way to reduce the bias (bias adjustment or bias correction)
\end{itemize}

\end{frame}


\begin{frame}{Deriving the matching bias}
	
  \vspace{-5mm}
  $$
		\widehat{\delta}_{ATT} = \frac{1}{N_T} \sum_{D_i=1} (Y_i - Y_{j(i)}),
  $$
  where each $i$ and $j(i)$ units are matched, $X_i \approx X_{j(i)}$ and $D_{j(i)}=0$. 
	 
  \bigskip
  Define potential outcomes and switching eq.
		\begin{eqnarray*}
      \mu^0(x) &=& E[Y | X=x,D=0] = E[Y^0 | X=x],\\
      \mu^1(x) &=& E[Y | X=x,D=1] = E[Y^1 | X=x],\\
      Y_i &=& \mu^{D_i}(X_i) + \varepsilon_i
		\end{eqnarray*}
\end{frame}

\begin{frame}{Deriving the matching bias}
  Substitute and distribute terms
  \begin{eqnarray*}
    \widehat{\delta}_{ATT} &=& \frac{1}{N_T} \sum_{D_i=1} (Y_i - Y_{j(i)}) \\
    &=& \frac{1}{N_T} \sum_{D_i=1} \left[ (\mu^1(X_i) + \varepsilon_i) - (\mu^0(X_{j(i)}) + \varepsilon_{j(i)}) \right] \\
    &=&  \frac{1}{N_T} \sum_{D_i=1} (\mu^1(X_i) - \mu^0(X_{j(i)})) + \frac{1}{N_T} \sum_{D_i=1}(\varepsilon_i - \varepsilon_{j(i)})
  \end{eqnarray*}
\end{frame}
		

\begin{frame}{Deriving the matching bias}
	
Difference between sample estimate and population parameter is:
		\begin{eqnarray*}
		\widehat{\delta}_{ATT} - \delta_{ATT} &=& \frac{1}{N_T} \sum_{D_i=1} \left( \mu^1(X_i) - \mu^0(X_{j(i)}) - \delta_{ATT}\right) \\
		&+& \frac{1}{N_T} \sum_{D_i=1} (\varepsilon_i - \varepsilon_{j(i)})
		\end{eqnarray*}
Algebraic manipulation and simplification:
		\begin{eqnarray*}
		\widehat{\delta}_{ATT} - \delta_{ATT} &=& \frac{1}{N_T} \sum_{D_i=1} \left( \mu^1(X_i) - \mu^0(X_i) - \delta_{ATT}\right) \\
		&+& \frac{1}{N_T} \sum_{D_i=1} (\varepsilon_i - \varepsilon_{j(i)}) \\
		&+& \frac{1}{N_T} \sum_{D_i=1} \left( \mu^0(X_i) - \mu^0(X_{j(i)}) \right).
		\end{eqnarray*}
\end{frame}


\begin{frame}{Deriving the matching bias}
	
Note $\widehat{\delta}_{ATT} - \delta_{ATT} \to 0$ as $N \to \infty$.
\pause However, 
$$E[ \sqrt{\frac{1}{N}} (\widehat{\delta}_{ATT} - \delta_{ATT})] = E[ \sqrt{\frac{1}{N}} ( \mu^0(X_i) - \mu^0(X_{j(i)}) ) | D=1].$$ 
\pause
Now consider the implications if $k$ is large:
\pause	\begin{itemize}
	\item The difference between $X_i$ and $X_{j(i)}$ converges to zero very slowly 
\pause	\item The difference $\mu^0(X_i) - \mu^0(X_{j(i)})$ converges to zero very slowly \pause
	\item $E[ \sqrt{\frac{1}{N}} (\mu^0(X_i) - \mu^0(X_{j(i)})) | D=1]$ may not converge to zero and an be very large! 
\pause	\item $E[ \sqrt{\frac{1}{N}} (\widehat{\delta}_{ATT} - \delta_{ATT})]$ may not converge to zero because the bias of the matching discrepancy is dominating the matching estimator! \pause
	\end{itemize}
 Bias is often an issue when we match in many dimensions
\end{frame}

\begin{frame}{Solutions to matching bias problem}
	
The bias of the matching estimator is caused by large matching discrepancies $||X_i - X_{j(i)}||$ which is virtually guaranteed by the curse of dimensionality.  However:
	\begin{enumerate}
	\item But the matching discrepancies are observed. We can always check in the data how well we're matching the covariates.

	\item For $\widehat{\delta}_{ATT}$ we can sometimes make the matching discrepancies small by using a large reservoir of untreated units to select the matches (that is, by making $N_C$ large).

  \item If the matching discrepancies are large, so we are worried about potential biases, we can apply bias correction techniques

	\end{enumerate}
\end{frame}

 
\begin{frame}{Matching with bias correction}
	
	\begin{itemize}
	\item Each treated observation contributes$$\alert{\mu^0(X_i)} - \mu^0(X_{j(i)})$$to the bias.
	\item Bias-corrected (BC) matching:
		\begin{eqnarray*}
		\widehat{\delta}_{ATT}^{BC} = \frac{1}{N_T} \sum_{D_i=1} \left[ (Y_i - Y_{j(i)}) - ( \widehat{\mu^0}(X_i) - \widehat{\mu^0}(X_{j(i)}) ) \right]
		\end{eqnarray*}where $\widehat{\mu^0}(X)$ is an estimate of $E[Y|X=x,D=0]$.  For example using OLS but other maybe too (neural nets?).  
	\item Under some conditions, the bias correction eliminates the bias of the matching estimator without affecting the estimator's variance.
	\end{itemize}
\end{frame}


\begin{frame}{Steps}

\begin{enumerate}
\item[1.  ]Regress $Y$ on $X$ with OLS except only use the control sample:
\begin{equation}
Y_j = \alpha + \beta X_j + \varepsilon_j \nonumber
\end{equation}where $j$ are the units for which $D_j=0$.  
\end{enumerate}

\end{frame}

\begin{frame}{Steps}

\begin{enumerate}
\item[2. ] Use the fitted values $\widehat{\alpha}$ and $\widehat{\beta}$ to predict $\widehat{\mu}^0(X)$ for both the $i$ and the matched $j(i)$ units:
\begin{eqnarray}
\widehat{\mu}^0_i &=& \widehat{\alpha} + \widehat{\beta} X_i \nonumber \\
\widehat{\mu}^0_{j(i)} &=& \widehat{\alpha} + \widehat{\beta} X_{j(i)} \nonumber
\end{eqnarray}
\end{enumerate}

\end{frame}


\begin{frame}{Steps}

\begin{enumerate}
\item[3. ] Subtract $ \widehat{\mu}_i^0(X_i) - \widehat{\mu}_{j(i)}^0(X_{j(i)})$, our estimate of the selection bias caused by matching discrepancies, from the sample estimate of the $ATT$: $$\widehat{\delta}_{ATT}^{BC} = \dfrac{1}{N_T} \sum_{D_i=1} \bigg [ (Y_i - Y_{j(i)}) - \Big(\widehat{\mu}^0(X_i) - \widehat{\mu}^0(X_{j(i)})\Big) \bigg ]$$
\end{enumerate}

\end{frame}


\begin{frame}{Steps}

\begin{enumerate}
\item[4. ] Estimate Abadie-Imbens robust standard error (Abadie and Imbens 2006; 2008; 2011)
\end{enumerate}

\end{frame}



\begin{frame}[plain,shrink=0]
	\begin{center}
	\textbf{Bias adjustment in matched data}
	\end{center}
	
	\begin{table}
	\begin{tabular}{c|c|c|c|c}
	\hline
	\multicolumn{1}{c}{}&
	\multicolumn{2}{c}{Potential Outcome}&
	\multicolumn{1}{c}{}&
	\multicolumn{1}{c}{}\\
	\multicolumn{1}{c}{unit} &
	\multicolumn{1}{c}{under Treatment}&
	\multicolumn{1}{c}{under Control}&
	\multicolumn{1}{c}{}&
	\multicolumn{1}{c}{}\\
	\hline
	$i$ & $Y^1_i$ & $Y^0_i$ & $D_i$ & $X_i$ \\
	\hline
	1 & \textcolor{blue}{10} &  \textcolor{blue}{8} & 1 & \textcolor{blue}{3} \\
	2 & 4 &  			 1 &				 1 & 1 \\
	3 & \textcolor{green}{10} &   \textcolor{green}{9} & 1 & \textcolor{green}{10} \\
	\hline
	4 &  & \textcolor{blue}{8} & 0 & \textcolor{green}{4} \\
	5 &  & 			 1 & 0 &  				0 \\
	6 &  & \textcolor{green}{9} & 0 & \textcolor{green}{8} \\
	\hline
	\end{tabular}
	\end{table}
	
	
	\begin{flalign*}
		\only<1-3>{&\widehat{\delta}_{ATT} = \frac{10-8}{3} + \frac{4-1}{3}  + \frac{10-9}{3}  = 2&} \\
		\only<2-3>{&\text{For the bias correction, estimate }\widehat{\mu^0}(X) = \widehat{\beta_0} + \widehat{\beta_1}X = 2+X}\\
		\only<3-3>{&\widehat{\delta}_{ATT} = \frac{ (10-8) - ( \widehat{\mu^0}(3) - \widehat{\mu^0}(4)) }{3}  + \frac{ (4-1)  -( \widehat{\mu^0}(1) - \widehat{\mu^0}(0)) }{3}  \\
		&+ \frac{ (10-9)  - ( \widehat{\mu^0}(10) - \widehat{\mu^0}(8))}{3} = 1.33}
	\end{flalign*}

\end{frame}


\begin{frame}{Matching bias: Implications for practice}
	
\emph{Matching} bias arises because of the effect of large matching discrepancies on $\mu^0(X_i) - \mu^0(X_{j(i)})$ due to a lack of common support. To minimize matching discrepancies:
	\begin{enumerate}
	\item Use a small $M$ (e.g., $M=1$). Larger values of $M$ produce large matching discrepancies.
	\item Use matching with replacement.  Because matching with replacement can use untreated units as a match more than once, matching with replacement produces smaller matching discrepancies than matching without replacement.
	\item Try to match covariates with a large effect on $\mu^0(\cdot)$ particularly well.
	\end{enumerate}
\end{frame}

\begin{frame}{Large sample distribution for matching estimators}
	
	\begin{itemize}
	\item Cannot use the bootstrap, so Abadie and Imbens derived the variance (Abadie and Imbens 2008)
	\item Matching estimators have a Normal distribution in large samples (provided the bias is small):
		\begin{eqnarray*}
		\sqrt{N_T} (\widehat{\delta}_{ATT} - \delta_{ATT}) \xrightarrow{d} N(0,\sigma^2_{ATT})
		\end{eqnarray*}
	\item For matching without replacement, the ``usual'' variance estimator:
		\begin{eqnarray*}
		\widehat{\sigma}^2_{ATT} = \frac{1}{N_T} \sum_{D_i=1} \left( Y_i - \frac{1}{M} \sum_{m=1}^M Y_{j_m(i)} - \widehat{\delta}_{ATT} \right)^2,
		\end{eqnarray*}is valid.
	\end{itemize}
\end{frame}

\begin{frame}{Large sample distribution for matching estimators}
	
	\begin{itemize}
	\item For matching with replacement:
		\begin{eqnarray*}
		\widehat{\sigma}^2_{ATT} &=& \frac{1}{N_T} \sum_{D_i=1} \left( Y_i - \frac{1}{M} \sum_{m=1}^M Y_{j_m(i)} - \widehat{\delta}_{ATT} \right)^2 \\
		&+& \frac{1}{N_T} \sum_{D_i=0} \left( \frac{K_i(K_i-1)}{M^2} \right) \widehat{var}(\varepsilon | X_i,D_i=0)
		\end{eqnarray*}where $K_i$ is the number of times observation $i$ is used as a match.
	\item $\widehat{var}(Y_i | X_i,D_i=0)$ can be estimated also by matching.  For example, take two observations with $D_i=D_j=0$ and $X_i \approx X_j$, then
		\begin{eqnarray*}
		\widehat{var}(Y_i | X_i,D_i=0) = \frac{(Y_i-Y_j)^2}{2}
		\end{eqnarray*}is an unbiased estimator of $\widehat{var}(\varepsilon_i | X_i,D_i=0))$
	\end{itemize}
\end{frame}


\subsection{Propensity scores}


\begin{frame}{Propensity score as dimension reduction}
	
	\begin{itemize}
	\item Curse of dimensionality makes matching on $K$ covariates challenging as the dimensions grew with $K$ (and continuous covariates had no exact matches)
	\item Rubin (1977) and Rosenbaum and Rubin (1983) developed the propensity score method to reduce $K$ covariates into a single scalar without loss of information
	\item Insofar as treatment is random conditional on $K$ covariates, then it will also be random conditional on propensity score 
	\item Variety of ways to incorporate the propensity score, but first we describe the propensity score as a dimension reduction method 
	\end{itemize}
	
\end{frame}


\begin{frame}{What's the propensity score for?}

\begin{itemize}

\item Once you obtain the propensity score, you typically use it to carefully explore problems with common support with respect to your selection parameter of interest
\item Estimators abound and can be a little bewildering so to summarize them:
	\begin{enumerate}
	\item Match units from one group to another using the propensity score (with various rules for finding how close to be)
	\item Weighting by the inverse propensity score
	\end{enumerate}
\item Variety of techniques to derive standard errors from parametric and bootstrapping
\item Can even introduce ``doubly robust'' methods to deal with matching bias like we did with bias correction

\end{itemize}

\end{frame}




\begin{frame}{Formal Definition}
	
	\begin{block}{Definition of Propensity score}
	A propensity score is a number bounded between 0 and 1 measuring the probability of treatment assignment conditional on a vector of confounding variables: $p(X)=Pr(D=1 | X)$
	\end{block}
	

\end{frame}

\begin{frame}{Propensity score theorem}
	
	\begin{block}{Propensity score theorem}
	If $(Y^1,Y^0)\independent{D}|X$ (full unconfoundedness), then $(Y^1,Y^0)\independent{D} | \rho(X)$ where $\rho(X)=Pr(D=1|X)$, the propensity score
	\end{block}
	
	\begin{itemize}
	\item Conditioning on the propensity score is enough to have independence between $D$ and $(Y^1,Y^0)$ (Rosenbaum and Rubin 1983)
	\item With full unconfoundedness, you can estimate the ATE (ATT recall needs weak unconfoundedness)
	\end{itemize}
\end{frame}

\begin{frame}{True vs Estimating Propensity Score}


\begin{itemize}
\item In an experiment, we know the true propensity score because we controlled it -- could be uniform, could be different by groups
\item Outside an experiment, we don't know it
	\begin{itemize}
	\item We use DAGs or hunches to select covariates, careful not to include outcomes or colliders
	\item But we may still not know the functional form
	\end{itemize}
\item Often then people will use higher order terms and interactions to provide flexibility but it does technically have misspecification biases
\item This is an area most likely where machine learning could greatly enhance the estimation
\end{itemize}

\end{frame}

\begin{frame}{Estimating with the Propensity Score}

\begin{itemize}

\item All the propensity score will do is collapse or ``reduce'' the dimensions of X from $K$ to a single scalar, but then what?
\item Weight or impute -- only things you can do
\item We will start with weighting and then move into imputation after
\item Inverse probability weighting to estimate the ATT

\end{itemize}

\end{frame}



\begin{frame}{Step 1: Estimate the propensity score}

		\begin{itemize}
		\item Estimate the conditional probability of treatment using probit or logit model (or ML) $$Pr(D_i=1|X_i) = F(\beta X_i)$$
		\item Use the estimated coefficients to predict the propensity score for each unit $i$$$\widehat{\rho}_i(X_i) = \widehat{\beta} X_i$$
		\item Note that each unit $i$ now has a predicted probability of treatment given the values of their covariates relative to everyone else's 
		\item Frequentist probability -- you've basically just obtained the likelihood someone who ``looks like you'' would be treated (regardless of whether you were in fact treated)
		\end{itemize}
\end{frame}




\begin{frame}{Step 2: Estimation of ATT with IPW}

\begin{itemize}
	\item IPW uses the estimated propensity score to reweight the outcomes (e.g., Robins and Rotnitzky 1995, Imbens 2000, Hirano and Imbens 2001)
	\item IPW is non-parametric -- you are just taking averages and multiplying by weights
	\item There are also fewer implementation choices -- you aren't choosing how many neighbors to include, how far away a neighbor can be -- but you still have to closely examine common support
	\item There are bias adjustment methods called double robust where you combine imputing counterfactuals with weighting by the propensity score
\end{itemize}

\end{frame}
	
	

\begin{frame}{Step 2: Estimation of ATT with IPW}
	
		\begin{block}{Estimating ATT with IPW}
	Given $Y^0 \independent{D}|X$ and weak common support, then
		\begin{eqnarray*}
		\delta_{ATT}&=&E[Y^1-Y^0|D=1] \\
		&=& \frac{1}{Pr(D=1)} \cdot  E \left[ Y \cdot \frac{D-\rho(X)}{1-\rho(X)} \right]
		\end{eqnarray*}
	\end{block}Notice that when $D=1$, the outcome is not weighted, but when $D=0$ it is. You're missing the $Y^0$ for the treatment, not $Y^1$ so you the treatment group $Y$ values alone and weight ``up'' or ``down'' the comparison groups by their propensity scores

\end{frame}


\begin{frame}{Step 3: Standard Errors}
	
		
Standard errors can be constructed a few different ways:
	\begin{itemize}
	\item We need to adjust the standard errors for first-step estimation of $\rho(X)$
		\begin{itemize}
		\item Parameteric first step: Newey and McFadden (1994)
		\item Non-parametric first step: Newey (1994)
		\end{itemize}
	\item IPW is a smooth estimator which means the bootstrap is valid for inference  (Adudumilli 2018 and Bodory et al. 2020) 
	\end{itemize}
\end{frame}


\begin{frame}{Check for common support using histograms}

\begin{itemize}

\item Recall that the matching/weighting methods all require unconfoundedness \emph{and} common support, and whereas the first is not testable, the latter is
\item Assessing whether there are units it both groups for whichever parameter you're focused on is simple with propensity score -- just create histograms of the propensity score distribution for treatment and control
\item Crump, et al. (2009) suggest keeping propensity scores within the interval [0.1,0.9] (``trimming'') but any trimming will drop units and dropping units means moving away from the parameter
\item ``Parameters first!''

\end{itemize}

\end{frame}

\begin{frame}{Assessing overlap}

\begin{figure}[!t]\centering
\includegraphics[scale=0.12]{./lecture_includes/pscore_overlap}
\end{figure}

\end{frame}

\begin{frame}{Estimating ATE with IPW}

\begin{itemize}
\item \emph{Any} of the parameters including the ATE are possible with propensity scores

\item Assumptions are strongest for ATE:  full unconfoundedness and full support versus weak versions of both 

\item You should only pursue the ATE if that is your preferred estimate, which also requires full unconfoundedness/support to be believable

\end{itemize}

\end{frame}
	


\begin{frame}{Estimating ATE with IPW}

	
		\begin{block}{Estimating ATE with IPW}
	Given $Y^1,Y^0 \independent{D}|X$ and common support, then
		\begin{eqnarray*}
		\delta_{ATE}&=&E[Y^1-Y^0] \\
		&=&E \left[ Y \cdot  \frac{D - \rho(X)}{\rho(X) \cdot (1-\rho(X))} \right]
		\end{eqnarray*}
	\end{block}

Notice that since treated units are missing counterfactuals, but so are controls, all of the data is weighted for the ATE (not just the controls for ATT)

\end{frame}


\begin{frame}{Inverse Probability Weighting}

	\begin{proof}
	\begin{eqnarray*}
	E \left[ Y \cdot \frac{D-\rho(X)}{\rho(X)(1-\rho(X))} \Big\vert X \right] &=& E \left[ \frac{Y}{\rho(X)} \Big\vert X,D=1 \right] \rho(X) \\
	&& + E\left[ \frac{-Y}{1-\rho(X)} \Big\vert X,D=0 \right](1-\rho(X)) \\
	&=& E[Y|X,D=1] - E[Y|X,D=0]
	\end{eqnarray*}and the results follow from integrating over $P(X)$ and $P(X|D=1)$.
	\end{proof}

\end{frame}

\begin{frame}{Other comments about propensity scores}

\begin{itemize}

\item Only other comments to make is that you can get outlier weights
\item If the propensity score is very high for a control group in the ATT, the weight can explode
\item Certain normalizations of the way the IPW is constructed to try and minimize those influences
\item Stata's \texttt{teffects} or R's \texttt{ipw} both let you estimate parameters using IPW and get standard errors

\end{itemize}

\end{frame}


\begin{frame}{Double robust estimators}

\begin{itemize}
\item You can have the right covariates but the wrong model and unbiasedness requires the correct model
\item What if you had a way to control for the covariates using propensity scores and something else like regression?
\item Buys you some insurance against model misspecification if such a thing existed
\end{itemize}

\end{frame}

\begin{frame}{Double robust estimators}

\begin{itemize}
\item Lots of papers began to try and address the model misspecification problem by combining propensity scores with other methods called ``double robust'' (Robins and Rotnizky 1995; Hirano and Imbens 2001)
\item Basic idea in all of them was to control for covariates twice \emph{at the same time} without paying a price
\item We say that estimators combining regression with IPW are double robust so long as
	\begin{itemize}
	\item The regression for the outcome is properly specified, or
	\item The propensity score is properly specified
	\end{itemize}
\item We give ourselves two chances to get it right (either/or not both/and) but if neither is properly specified, then you didn't really gain much
\end{itemize}

\end{frame}





\begin{frame}{Propensity score matching}
	
	\begin{itemize}
	\item Matching, or ``imputation'', is another way that utilizes the $\widehat{\rho_i}(X_i)$
	\item Matching estimation based on the propensity score has the same first step as IPW, but not the second and third steps
	\item Common support starts to be more complex with imputation methods because you will need to decide how far away from a unit's own propensity score is a tolerable distance to be considered a ``neighbor''
	\end{itemize}

\end{frame}




\begin{frame}{Standard matching strategy}
	
	\begin{itemize}
	\item Pair each treatment unit $i$ with one or more \emph{comparable} control group unit $j$, where comparability is in terms of proximity, or distance, to the estimated propensity score
	\item Impute the unit's missing counterfactual outcome $Y_{i(j)}$ based on the unit or units chosen in the previous step
	\item If more than one are ``nearest neighbors'', then use the neighbors' weighted outcomes  $$Y_{i(j)} = \sum_{j \in C(i)} w_{ij}Y_j$$ where $C(i)$ is the set of neighbors with $W=0$ of the treatment unit $i$and $w_{ij}$ is the weight of control group units $j$ with $\sum_{j \in C(i)} w_{ij} = 1$
	\end{itemize}
\end{frame}


\begin{frame}{Imputing the counterfactuals}
	
	Let the ATT be our parameter of interest: $$E[Y^1_i | D_i=1] - E[Y^0_i | D_i=1]$$We estimate it as follows  $$\widehat{ATT} = \frac{1}{N_T}  \sum_{i:D_i=1} \bigg [Y_i - Y_{i(j)} \bigg ]$$where $N_T$ is the number of matched treatment units in the sample. Note the difference between \emph{imputation} and IPW -- the only weight here is $\frac{1}{N_T}$

\end{frame}

\begin{frame}{Matching methods}
	
	\begin{itemize}
	\item The probability of observing two units with \emph{exactly} the same propensity score is in principle zero if $Pr(X=x)$ is continuous
	\item Several matching methods have been proposed in the literature, but the most widely used are:
		\begin{itemize}
		\item Stratification matching
		\item Nearest-neighbor matching (with or without caliper)
		\item Radius matching
		\item Kernel matching
		\end{itemize}
	\item Typically, one treatment unit $i$ is matched to several control units $j$, but sometimes one-to-one matching is used
	\end{itemize}
	
\end{frame}


\begin{frame}{Stratification}
	
	\begin{itemize}
	\item Stratification based on the propensity score is a multi step process that bears resemblance to the stratification/subclassification method proposed by Cochran (1968)
	\item  Method uses brute force to achieve the balancing property discussed earlier, which is then used with weighted differences in means within propensity score ``strata''
	\item Dehejia and Wahba (2002) used stratification matching in their seminal paper 
	\end{itemize}
	
\end{frame}


\begin{frame}{Stratification: Achieving Balance}
	
First create ``propensity score strata'' inside which you have balanced covariates
	\begin{enumerate}
		\item Sort the data by propensity score and divide into groups of observations with similar propensity scores (e.g., percentiles)
		\item Within each strata, test (e.g., t-test) whether the means of the $k$ covariates are equal between treatment and control
		\item If so, then stop.  If not, it means the covariates aren't balanced \emph{within that propensity score strata} so then divide that strata in half and repeat step 2
		\item If a particular covariate is unbalanced for multiple groups, modify the initial logit or probit equation by including higher order terms and/or interactions with that covariate and repeat
		\end{enumerate}

\end{frame}

\begin{frame}{Propensity score matching}
	
	\begin{itemize}
	\item Next we review explicit imputation based on the propensity score or what is sometimes called propensity score matching
	\item King and Nielsen (2019) is a critique of using propensity scores \emph{for matching} (i.e., imputation)
	\item But not a critique of the propensity score itself or to stratification, regression adjustment, or IPW
	\item Issues raised have to do with forced balance through trimming and a myriad of other common choices made by the researcher
	\end{itemize}
	
\end{frame}

\begin{frame}{Ad hoc user choices introduce bias}

\begin{quote}
	
```[The] more balanced the data, or the more balance it becomes by [trimming] some of the observations through matching, the more likely propensity score matching will degrade inferences.'' -- King and Nielsen (2019)

\end{quote}
	
\end{frame}



\begin{frame}{Nearest Neighbor}
	
	Pretty similar to covariate matching.  Formula is $$\widehat{ATT} = \frac{1}{N_T} \sum_{i:D_i=1} \bigg [ Y_i - \sum_{j \in C(i)_M} w_{ij}Y_j \bigg ]$$ 
		\begin{itemize}
		\item $N_T$ is the number of treated units $i$ and  $N_C$ is number of control units $j$
		\item $w_{ij}$ is equal to $\frac{1}{N_C}$ if $j$ is a control unit and zero otherwise
		\item And unit $j$ is chosen as a control for $i$ if it's propensity score is nearest to that of $i$
		\end{itemize}
\end{frame}

\begin{frame}{NN Matching: Bias vs. Variance}
	
How far away on the propensity score will you use is what makes some of the different types of matching proposed differ
		\begin{itemize}
		\item Matching just one nearest neighbor minimizes bias at the cost of larger variance
		\item Matching using additional nearest neighbors increases the bias but decreases the variance
		\end{itemize}
	
\end{frame}

\begin{frame}{NN Matching: Bias vs. Variance}
	
	 Matching with or without replacement
		\begin{itemize}
		\item with replacement keeps bias low at the cost of larger variance
		\item without replacement keeps variance low but at the cost of potential bias
		\end{itemize}
	
\end{frame}



\begin{frame}{Distance between treatment and control units}

\begin{itemize}
\item What was historically done was limiting ``distance''  through various \emph{ad hoc} choices
\item Imagine these choices as creating like a cowboy rope lasso that matches to everything inside that circle
\item There were two common ways for creating the circle -- caliper matching and radius matching. 
\end{itemize}

\end{frame}

\begin{frame}{Caliper matching}
	
	\begin{itemize}
	\item Caliper matching is a variation on NN matching that tries to build brakes into the algorithm as to avoid ``bad neighbors'' by imposing a tolerable maximum distance (e.g., 0.2 units in the propensity score away from a treatment unit $i$'s propensity score)
	\item Note -- this is a one-to-one imputation, and if there doesn't exist anybody in the control group unit $j$ within that ``caliper'', then treatment unit $i$ is discarded which as with all trimming changes the parameter we are estimating
	\item It's difficult to know what this caliper should be \emph{ex ante}, hence why I said it is somewhat \emph{ad hoc}
	\end{itemize}

\end{frame}


\begin{frame}{Radius matching}
	
	
		\begin{itemize}
		\item Each treatment unit $i$ is matched with the control group units whose propensity score are in a ``predefined neighborhood'' of the propensity score of the treatment unit.
		\item \textbf{All} the control units with $\widehat{\rho_j}(X_{j})$ falling within a radius $r$ from $\widehat{\rho}_i(X_i)$ are matched to the treatment unit $i$ -- this is what distinguishes it from calipers, and makes it more similar to covariate matching (Abadie and Imbens 2006, 2008)
		\item The smaller the radius, the better the quality of the matches, but the higher the possibility some treatment units are not matched because the neighborhood does not contain control group units $j$
		\end{itemize}
		
\end{frame}

\begin{frame}{Software}

\begin{itemize}
\item You can use -\texttt{teffects, psmatch}- to get at these two nearest neighbor approaches by setting the number of matches
\item You can use -\texttt{pscore2}- for stratification
\item You can use the \texttt{MatchIt} package in R
\end{itemize}

\end{frame}





\subsection{Regressions}

\begin{frame}{OLS with controls}

\begin{itemize}

\item Most common causal model, arguably, is OLS with covariate controls used to establish mean exogeneity and eliminate omitted variable bias

\begin{eqnarray*}
Y_{it} = \alpha + \delta D_{it} + \beta X_{it} + \varepsilon_{it}
\end{eqnarray*}

\item This as it turns out is related to what we've discovered but the ways that is different matter a lot for estimation and unbiasedness

\end{itemize}

\end{frame}


\begin{frame}{Interpreting OLS coefficients}

\begin{itemize}

\item OLS can do a lot of things, but one thing it does it find the best linear fit of the data
\item Finding the best linear fit is the same as finding the counterfactual, so to get OLS to serve that function is another task

		\begin{align*}
		Y_i = \beta_0 + \beta_1 X_i + u_i&\\
		\end{align*}

\item When we estimate this model, it can be easily visualized with data

\end{itemize}

\end{frame}

\begin{frame}{OLS Simulated Code}

\begin{figure}[!t]\centering
\includegraphics[scale=0.40]{./lecture_includes/ols_code}
\end{figure}

\end{frame}

\begin{frame}{OLS Plot and Line}

\begin{figure}[!t]\centering
\includegraphics[scale=0.4]{./lecture_includes/ols_line}
\end{figure}

\end{frame}

\begin{frame}{Bivariate vs multivariate regressions}

\begin{itemize}

\item Interpreting the OLS coefficient in bivariate regression also has a simple express as a scaled covariance:

 $$\widehat{\beta_1}=\frac{Cov(Y_i,X_i)}{Var(X_i)}$$
 
 \item But what about when we are looking at a multivariate regression -- too many dimensions to visualize it
 \item A trick called the Frisch-Waugh-Lovell (FWL) theorem will turn the multivariate regression into the simple bivariate one
\item FWL also helps us interpret $\widehat{\beta}_1$ when there are covariates
 
 \end{itemize}
 
 \end{frame}
 
 \begin{frame}{Applying FWL theorem}

\begin{itemize}
\item Angrist and Pischke (2009) call FWL the ``regression anatomy theorem'', as does Filoso (2013)
\item Filoso (2013) has an excellent simplification of what is a historically complex proof
\end{itemize}

\end{frame}



\begin{frame}{Applying FWL theorem}

	\begin{itemize}
	\item FWL theorem concerns multiple linear regression and helps us interpret regression coefficients with multiple controls.
	\item Can we estimate the causal effect of family size on labor supply by regressing labor supply (\texttt{Y}) on family size (\texttt{X})?
		\begin{eqnarray*}
		Y_i = \beta_0 + \beta_1 X_i + u_i&\\
		\end{eqnarray*}
	\item If family size is random, then number of kids is uncorrelated with the unobserved error term, which means we can interpret $\widehat{\beta_1}$ as the ATE.
	\item But how do we interpret $\widehat{\beta_1}$ if \texttt{family size} is non-random?  
	\end{itemize}
\end{frame}


\begin{frame}{Applying FWL theorem}
	
	\begin{itemize}
	\item Assume that family size is random once we condition on race, age, marital status and employment.  Then the model is:
		\begin{eqnarray*}
Y_i &=& \beta_0 + \beta_1 X_i + \gamma_1 \texttt{White}_i + \gamma_2 \texttt{Married}_i \\
		& & + \gamma_3 \texttt{Age}_i + \gamma_4 \texttt{Employed}_i + u_i
		\end{eqnarray*}
	\item If we want to estimate average causal effect of family size on labor supply, we will need two things: 
		\begin{enumerate}
		\item a data set with all $6$ variables; 
		\item Number of kids ($X_i$) must be randomly assigned conditional on the other $4$ variables
		\end{enumerate}
	\item How do we interpret $\widehat{\beta_1}$ with the additional controls?
	\end{itemize}
	
\end{frame}


\begin{frame}{Applying FWL theorem}

\begin{itemize}
\item FWL shows you that in a multivariate regression, any one coefficient fitted can be reconceived as a simple scaled covariance of two variables
\item It says that $\widehat{\beta_1}$ is simply a scaled covariance with the $\tilde{X_1}$ residual used instead of the actual data $X$
\end{itemize}

\end{frame}


		

\begin{frame}{FWL Theorem}

	\begin{block}{FWL Theorem }
	Assume your main multiple regression model of interest:$$y_i=\beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki} + \dots + \beta_Kx_{Ki} + e_i$$ and an auxiliary regression in which the variable $x_{1i}$ is regressed on all the remaining independent variables$$x_{1i} = \gamma_0 + \gamma_{k-1}x_{k-1 i}+\gamma_{k+1}x_{k+1 i} + \dots + \gamma_Kx_{Ki}+f_i$$and $\tilde{x}_{1i} = x_{1i} - \widehat{x}_{1i}$ being the residual from the auxiliary regression. The parameter $\beta_1$ can be rewritten as:$$\beta_1=\frac{Cov(y_i,\tilde{x}_{1i})}{Var(\tilde{x}_{1i})}$$

	\end{block}

\end{frame}


\begin{frame}[plain, shrink=20]

\bigskip

	\begin{block}{FWL Proof  (Proof by Filoso 2013)}
	To prove the theorem, note $E[\tilde{x}_{ki}]=E[x_{ki}]-E[\widehat{x}_{ki}]=E[f_i]$, and plug $y_i$ and residual $\tilde{x}_{ki}$ from $x_{ki}$ auxiliary regression into the covariance $cov(y_i, \tilde{x}_{ki})$
	\begin{eqnarray*}
	\beta_k &=& \frac{ cov(y_i, \tilde{x}_{ki})}{var(\tilde{x}_{ki})} \\
			 &=& \frac{ cov(\beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki} + \dots + \beta_Kx_{Ki} + e_i, \tilde{x}_{ki})}{var(\tilde{x}_{ki})} \\
			&=& \frac{ cov(\beta_0 + \beta_1x_{1i} + \dots + \beta_kx_{ki} + \dots + \beta_Kx_{Ki} + e_i,f_i)}{var(f_i)}
	\end{eqnarray*}
	\begin{enumerate}
	\item Since by construction $E[f_i]=0$, it follows that the term $\beta_0E[f_i]=0$.
	\item Since $f_i$ is a linear combination of all the independent variables with the exception of $x_{ki}$, it must be that$$\beta_1E[f_ix_{1i}] = \dots = \beta_{k-1}E[f_ix_{k-1 i}] = \beta_{k+1}E[f_ix_{k+1 i}] = \dots = \beta_KE[f_ix_{KI}] = 0$$
	\end{enumerate}
	\end{block}
\end{frame}


\begin{frame}[plain, shrink=20]

	\begin{block}{FWL Proof  (Proof by Filoso 2013)}
		\begin{enumerate}\addtocounter{enumi}{2}
		\item Consider now the term $E[e_if_i]$.  This can be written as:
			\begin{eqnarray*}
			E[e_if_i] &=& E[e_if_i] \\
			&=& E[e_i\tilde{x}_{ki}] \\
			&=& E[e_i(x_{ki} - \widehat{x}_{ki})] \\
			&=& E[e_ix_{ki}] - E[e_i\tilde{x}_{ki}]
			\end{eqnarray*}Since $e_i$ is uncorrelated with any independent variable, it is also uncorrelated with $x_{ki}$: accordingly, we have $E[e_ix_{ki}]=0$. With regard to the second term of the subtraction, substituting the predicted value from the $x_{ki}$ auxiliary regression, we get$$E[e_i\tilde{x}_{ki}] = E[e_i(\widehat{\gamma_0} + \widehat{\gamma_1}x_{1i} + \dots + \widehat{\gamma}_{k-1}x_{k-1}i+\widehat{\gamma}_{k+1}x_{k+1 i} + \dots + \widehat{\gamma}_Kx_{Ki})]$$Once again, since $e_i$ is uncorrelated with any independent variable, the expected value of the terms is equal to zero.  Then, it follows $E[e_if_i]=0$.
		
		\end{enumerate}
	\end{block}

\end{frame}

\begin{frame}[plain, shrink=25]

	\begin{block}{FWL Proof  (Proof by Filoso 2013)}
		\begin{enumerate}\addtocounter{enumi}{3}
		\item The only remaining term is $E[\beta_kx_{ki}f_i]$ which equals $E[\beta_kx_{ki}\tilde{x}_{ki}]$ since $f_i=\tilde{x}_{ki}$. The term $x_{ki}$ can be substituted using a rewriting of the auxiliary regression model, $x_{ki}$, such that$$x_{ki} = E[x_{ki} | X_{-k}] + \tilde{x}_{ki}$$This gives
			\begin{eqnarray*}
			E[\beta_kx_{ki}\tilde{x}_{ki}] &=& E[\beta_kE[\tilde{x}_{ki}(E[x_{ki}|X_{-k}]+\tilde{x}_{ki})]] \\
			&=& \beta_kE[\tilde{x}_{ki}(E[x_{ki}|X_{-k}]+\tilde{x}_{ki})] \\
			&=&\beta_k\{E[\tilde{x}^2_{ki}] + E[(E[x_{ki}|X_{-k}]\tilde{x}_{ki})]\} \\
			&=& \beta_k var(\tilde{x}_{ki})
			\end{eqnarray*}which follows directly from the orthogonoality between $E[x_{ki} | X_{-k}]$ and $\tilde{x}_{ki}$. From previous derivations we finally get$$cov(y_i,\tilde{x}_{ki}) = \beta_kvar(\tilde{x}_{ki})$$which completes the proof. \qedhere
		\end{enumerate}
	\end{block}

\end{frame}

\begin{frame}{FWL}

\begin{itemize}
\item Let's review the FWL ``partialing out'' interpretation of OLS estimated $\widehat{\beta_1}$ in code
\item We will use the fwl.do at /Labs/Matching for this
\end{itemize}

\end{frame}



\begin{frame}{Core OLS assumptions}

\begin{itemize}
\item Most common causal model is OLS with covariates is additive in controls: $$Y_i = \alpha + \delta D_i + \beta X_i + \gamma Z_i + \varepsilon_i$$
\item Which average treatment effect parameter does $\widehat{\delta}$ estimate?  What assumptions are imposed by exogeneity?
\item Next we explore the OLS model and its ability (or not) to recover a given parameter of interest
\item This hopefully is where I'll be able to convince you of how important it is that you identify ahead of time \emph{which} causal effect
\end{itemize}

\end{frame}

\begin{frame}{OLS Assumptions}

\begin{itemize}

\item Typically we assume that the mean error is zero conditional on all covariates, called exogeneity
\item This is a pregnant assumption as it turns out
\item Imbens and Rubin discussed it in their book on causal inference from 2015
\item I'll pull out their quote and proof but we will then discuss some other materials

\end{itemize}

\end{frame}


\begin{frame}{OLS Assumptions}

	\begin{quote} 
	``In many empirical studies in social sciences, causal effects are estimated through linear regression, where, typically it is implicitly assumed that in the super-population, $$E[Y_i^D | X_i] = \alpha + \delta_{sp} \cdot D + X_i \beta$$ for some values of the three unknown parameters, $\alpha$, $\delta_{sp}$ and $\beta$ where $\delta_{sp} = E_{sp} [ Y_i^1 - Y_i^0]$.''
	\end{quote}
	
\end{frame}

\begin{frame}{What about OLS?  (Imbens and Rubin 2015)}

\begin{quote}
``Defining $\varepsilon_i = Y_i - \delta_{sp} \cdot D_i - X_i \beta$ so that we can write $$Y_i = \alpha + \delta_{sp} \cdot D_i + X_i \beta  + \varepsilon_i$$ it is then assumed that $$\varepsilon_i \independent D_i, X_i$$This assumption is often referred to as \textbf{exogeneity} of the treatment (and the pre-treatment variables) in the econometrics literature.''

\end{quote}

\end{frame}

\begin{frame}{OLS Assumptions}

\begin{quote}
``The regression function is interpreted as a causal relation, in our sense of the term ``causal'', namely that if we manipulate the treatment $D_i$, then tht outcome would change in expectation by an amount $\delta_{sp}$.  Hence in the potential outcomes formulation, we have 
\begin{eqnarray*}
Y_i^0 &=& \alpha + X_i \beta + \varepsilon_i \\ 
Y_i^1 &=& Y_i^0 + \delta_{sp}
\end{eqnarray*} 

\end{quote}
	
\end{frame}

\begin{frame}{OLS Assumptions}

\begin{quote}
``Then, because $\varepsilon_i$ is a function of $Y^0_i$ and $X_i$ given the parameters, $$Pr(D_i=1 | Y_i^0, Y_i^1 X_i ) = Pr(D_i | \varepsilon_i, X_i),$$ and by exogeneity of the treatment indicator, we have $$Pr(D_i|\varepsilon_i, X_i) = Pr(D_i | X_i)$$ and thus [conditional independence] holds.'' 
\end{quote}

\end{frame}

\begin{frame}{OLS Assumptions}

\begin{quote}
``However, the exogeneity assumption combines unconfoundedness with functional form and constant treatment effect assumptions that are quite strong, and arguably unnecessary.'' -- Imbens and Rubin (2015)
\end{quote}

\end{frame}




\begin{frame}{Constant Treatment Effects and Linearity}

Most commonly used method is OLS where the outcome is an additive model of the observed outcome, $Y$, on the treatment, $D$, and covariates, $X$ like:

\begin{eqnarray*}
Y_{i} &=& \alpha + \delta D_i + \beta_1 X_i +  \varepsilon_i \\
\end{eqnarray*}

Take conditional expectations 

\begin{eqnarray*}
E[Y_i | D_i = 1, X_i] &=& \alpha + \delta E[D_i | D_i=1, X_i] + \beta_1 E[X_i | D_i=1, X_i] \\
E[Y_i | D_i = 0, X_i] &=& \alpha + \delta E[D_i | D_i=0, X_i] + \beta_1 E[X_i | D_i=0, X_i] 
\end{eqnarray*}

\end{frame}

\begin{frame}{Constant Treatment Effects and Linearity}

Replace realized variables with potential notation (both outcomes and covariates):
\begin{eqnarray*}
E[Y^1_i | D_i = 1,X_i] &=& \alpha + \delta + \beta_{11} E[X^1_i | D_i=1, X_i] \\
E[Y^0_i | D_i = 0,X_i] &=& \alpha + \beta_{01} E[X^0_i | D_i=0, X_i] 
\end{eqnarray*}

\bigskip

May seem somewhat unorthodox to also let $X$ have potential status, but you'll see why in a minute


\end{frame}


\begin{frame}{Constant Treatment Effects and Linearity}

OLS Estimator is a simple difference in conditional means:

\begin{eqnarray*}
\widehat{\delta} &=& E[Y^1_i | D_i = 1, X_i]  - E[Y^0_i | D_i = 0, X_i]  \\
\widehat{\delta} &=& \bigg (\alpha + \delta E[D_i | D_i=1, X_i] + \beta_{11} E[X^1_i | D_i=1, X_i] \bigg ) \\
&& - \bigg (\alpha + \delta E[D_i | D_i = 0, X_i] + \beta_{01} E[X^0_i | D_i = 0, X_i] \bigg ) \\
&=& \delta + \beta_{11} E[X^1_i | D_i =1, X_i] - \beta_{01} E[X^0_i | D_i = 0, X_i] 
\end{eqnarray*}

\bigskip

OLS model requires three things: (1) linearity, (2) covariates to be independent of treatment status (i.e., treatment cannot cause covariates to change), (3) $\beta_{11}=\beta_{01}$ (homogenous treatment effects with respect to $X$).  

\end{frame}

\begin{frame}{Simulation}

\begin{itemize}

\item Following code will maintain linearity but have common support violation to show OLS does not require common support, but does require linearity (it extrapolates based on functional form which is quite spectacular with correct model)
\item I will also create heterogenous treatment effects 
\item But I will also violate the previous requirement that $\beta_{11}=\beta_{01}$ so that you can see the bias that forms on average across 1,000 simulations
\item Will show a variety of estimators and specifications so that we see how to recover causal parameters with regression and matching
\end{itemize}

\end{frame}

\begin{frame}{Heterogenous Treatment Effects wrt $X$}

\begin{figure}[!t]\centering
\includegraphics[scale=0.27]{./lecture_includes/stata_matching_code}
\end{figure}

\end{frame}

\begin{frame}{Parameters}

\begin{itemize}

\item Ordinarily we look at the coefficient on the treatment dummy to obtain an estimate
\item But we have two parameters: the ATE is \$2500 but the ATT is \$1980
\item How do we get both of them? Let's look at what people usually do
\item 1,000 simulations of DGP with regression estimates plotting coefficient on treatment dummy: first with just age and GPA, second with the precise model used for $Y^0$ (but not $Y^1$)

\end{itemize}

\end{frame}


\begin{frame}{Constant Treatment Effects and Linearity}

\begin{figure}[!t]\centering
\includegraphics[scale=0.45]{./lecture_includes/stata_reg_constant}
\end{figure}

\end{frame}


\begin{frame}{Coefficient on Treatment Dummy is Wrong}

\begin{figure}[!t]\centering
\includegraphics[scale=0.1]{./lecture_includes/combined_kernels.jpg}
\end{figure}

\end{frame}

\begin{frame}{Commentary}

\begin{itemize}

\item Three ifs and a then:

	\begin{itemize}
	\item \emph{If} unconfoundedness held, and 
	\item \emph{if} the potential outcome model was linear, and
	\item  \emph{if} the treatment effect had been homogenous with respect to age and GPA, 
	\item \emph{then} the coefficient on the treatment variable would have been the ATE 
	\end{itemize}
	
\item But it wasn't because homogeneity with respect to $X$ was not true (recall $Y(0)$ coefficients were not the same as $Y(1)$ coefficients)
\item So what had we done?  Bear with me but this will pay off
\end{itemize}

\end{frame}

\begin{frame}{Heterogenous treatment effects}

Write down a simplified version of the DGP from the code:

\begin{eqnarray*}
Y^0_i &=& \alpha + \beta_{01} X^0_i + \varepsilon_i \\
Y^1_i &=& \alpha + \beta_{01} X^0_i + \delta D_i + \beta_{11} X^1_i \times D_i + \varepsilon_i
\end{eqnarray*}

\bigskip

Notice that the setup before, $X_i$ had a different effect on $Y^0$ than it did on $Y_i^1$ -- that's because of heterogenous treatment effects with respect to conditioning set. 

\end{frame}

\begin{frame}{Heterogenous treatment x`effects}

Take conditional expectations of the \emph{potential} outcomes:

\begin{eqnarray*}
E[Y^0_i | D_i=1, X_i] &=& \alpha + \beta_{01} E[X^0_i] \\
E[Y^1_i | D_i=1, X_i] &=& \alpha + \beta_{01} E[X^0_i] + \delta + \beta_{11} E[X^1_i \times D_i | D_i=1,X_i^1]
\end{eqnarray*}

\bigskip

Average treatment effect is: 

\bigskip

\footnotesize

\begin{eqnarray*}
E[Y_i^1 | D_i, X^1_i] - E[Y^0_i | D_i, X^0_i] &=& \bigg (  \alpha + \beta_{01} E[X^0_i] + \delta + \beta_{11} E[X^1_i \times D_i | D_i=1,X_i^1] \bigg ) \\
&& -  \bigg (\alpha + \beta_{01} E[X^0_i] \bigg ) \\
&=& \delta + \beta_{11}  E[X^1_i | D_i=1] 
\end{eqnarray*}assuming $X^1_i=X^0_i$. OLS model accounting for heterogeneity must be ``fully saturated''.


\end{frame}




\begin{frame}{Estimation}

Our saturated OLS model is:

\bigskip

\begin{eqnarray*}
Y_i = \alpha + \delta D_i + \beta_{01} X_i + \beta_{11} D_i \times X_i + \varepsilon_i
\end{eqnarray*}

$\widehat{\delta}$ is the ATE but the ATT is equal to $\widehat{\delta} + \beta_{11} E[X_i | D_i = 1]$ where $E[X_i | D_i=1]$ is the sample average of $X_i$ for the treatment group

\bigskip

We will estimate two models: (1) once with simplified but incorrectly specified saturated and (2) another with the correctly specified saturated model -- warning, it's a huge pain and you can easily mess it up even with just a few variables

\end{frame}



\begin{frame}{Misspecified Saturated OLS Regression}

\begin{figure}[!t]\centering
\includegraphics[scale=0.3]{./lecture_includes/saturated_reg3}
\end{figure}

\end{frame}


\begin{frame}{Misspecified Saturated OLS Regression}

\begin{figure}[!t]\centering
\includegraphics[scale=0.1]{./lecture_includes/combined_saturated1.jpg}
\end{figure}

\end{frame}

\begin{frame}{Comically Long Saturated OLS Regression}

\begin{figure}[!t]\centering
\includegraphics[scale=0.26]{./lecture_includes/stata_full_code}
\end{figure}

\end{frame}

\begin{frame}{Correctly Saturated OLS Regression}

\begin{figure}[!t]\centering
\includegraphics[scale=0.1]{./lecture_includes/combined_saturated2.jpg}
\end{figure}

\end{frame}

\begin{frame}{Regression adjustment}

\begin{itemize}

\item Notice how the same regression (fully interacted) led to \emph{both} the ATE and the ATT?
\item That means if you don't state ahead of time which parameter you want, how are you going to know how to set it up, and how will you know how to recover it?
\item Thankfully software exists that does this for you called ``regression adjustment'' by Wooldridge (2010) or Oaxaca-Blinder (Kline 2011; Graham and Pinto 2022) 
\item In Stata teffects, you can just the \texttt{ra} to get it -- see \texttt{comparison.do}

\end{itemize}

\end{frame}


\begin{frame}{Matching}

\begin{itemize}

\item Now let's estimate the ATT (\$1980) using nearest neighbor matching by minimizing Mahalanobis distance on age, GPA, polynomials and interaction
\item One line in Stata using \texttt{teffects} and only 1 match (variance is simple to estimate until we use matches multiple times, then the variance grows)
\item In R, the package is \texttt{Matching}, not sure in python
\end{itemize}

\end{frame}


\begin{frame}{Matching Estimation}

\begin{figure}[!t]\centering
\includegraphics[scale=0.1]{./lecture_includes/combined_kernels_maha.jpg}
\end{figure}

\end{frame}

\begin{frame}{Commentary}

\begin{itemize}

\item Technically I was only ``fully interacting'' -- full saturation would be to interact the treatment dummy with every value of the covariates yielding a huge number of parameters likely that cannot be estimated
\item Regression adjustment within -teffects- will do this for you
\item But note, with heterogeneity you have to use the fully saturated model (I just hadn't dummied every value of the covariates) to get both the ATE and the ATT
\item Otherwise you are imposing strong and unnecessary assumptions on the data that the treatment effects are the same for all values of $X$ and constant treatment effects so that the single coefficient is the ATE and the ATT

\end{itemize}

\end{frame}






\begin{frame}{Commentary}

\begin{itemize}

\item Unconfoundedness requires that you \emph{know} and \emph{include} all confounders to adjust comparisons when estimating treatment effects
\item Without a prior behavioral model guiding you, it's very hard to defend conditional independence (borderline disingenuous) 
\item If you are unwilling to use DAGs, you may want to ask yourself why you are comfortable running regressions with covariates?

\end{itemize}

\end{frame}

		  

\begin{frame}{Failure of econometric estimators (LaLonde 1986)}
	
	\begin{itemize}
	\item Evaluation of the Job Trainings Program (NSW) has a rich history in causal inference
	\item Bob LaLonde (passed away November 2015) was a Card and Ashenfelter student at Princeton whose job market paper evaluated, not NSW itself, but econometric methods one would use in something like NSW
	\item Dehejia and Wahba (1999; 2002) used LaLonde's data with propensity score matching and found they could recover known effects
	\item Critiques by Petra Todd, Jeff Smith and others followed which I'll summarize
	\end{itemize}
\end{frame}

\begin{frame}{Summarizing LaLonde (1986)}

		\begin{itemize}
		\item Very clever study that combined experimental and non-experimental data to ascertain whether popular econometric methods could recover unbiased effects when those effects were already known 
		\item Damning conclusion -- 1986 AER (it was LaLonde's JMP) found econometric methods failed to get the number right, and worse, failed to get the sign right
		\item Was a critical paper in the emerging ``credibility crisis'' within labor and helped fuel the type of work we now broadly consider to be design based causal inference
		\end{itemize}

\end{frame}

\begin{frame}[plain]
	\begin{center}
	LaLonde, Robert J. (1986). \myurlshort{http://business.baylor.edu/scott_cunningham/teaching/lalonde-1986.pdf}{``Evaluating the Econometric Evaluations of Training Programs with Experimental Data''}. \emph{American Economic Review}. 
	\end{center}
	
\underline{LaLonde's study} was \textbf{not} an evaluation of the NSW program, as that had been done, but rather an evaluation of econometric models done by:
		\begin{itemize}
		\item replacing the experimental NSW control group with non-experimental control group drawn from two nationally representative survey datasets: Current Population Survey (CPS) and Panel Study of Income Dynamics (PSID)
		\item estimating the average effect using non-experimental workers as controls for the NSW trainees 
		\item comparing his non-experimental estimates to the experimental estimates of \$900
		\end{itemize}
\end{frame}

\begin{frame}{LaLonde (1986)}

\begin{itemize}

	\item \underline{LaLonde's conclusion}: available econometric approaches were biased and inconsistent
		\begin{itemize}
		\item His estimates were way off and usually the wrong sign
		\item Conclusion was influential in policy circles and led to greater push for more experimental evaluations
		\end{itemize}

\end{itemize}

\end{frame}


\begin{frame}{Description of NSW Job Trainings Program}
	
The National Supported Work Demonstration (NSW), operated by Manpower Demonstration Research Corp in the mid-1970s:
	\begin{itemize}
	\item was a temporary employment program designed to help disadvantaged workers lacking basic job skills move into the labor market by giving them work experience and counseling in a sheltered environment
	\item was also unique in that it \textbf{randomly assigned} qualified applicants to training positions:
		\begin{itemize}
		\item \textbf{Treatment group}: received all the benefits of NSW program
		\item \textbf{Control group}: left to fend for themselves
		\end{itemize}
	\item admitted AFDC females, ex-drug addicts, ex-criminal offenders, and high school dropouts of both sexes
	\end{itemize}
\end{frame}

\begin{frame}{NSW Program}
	
	\begin{itemize}
	\item Treatment group members were:
		\begin{itemize}
		\item guaranteed a job for 9-18 months depending on the target group and site
		\item divided into crews of 3-5 participants who worked together and met frequently with an NSW counselor to discuss grievances and performance
		\item paid for their work
		\end{itemize}
	\item Control group members were randomized so the same
	\item Note: the randomization balanced observables and unobservables across the two arms, thus enabling the estimation of an ATE for the people who self-selected into the program
	\end{itemize}
\end{frame}

\begin{frame}{NSW Program}

\begin{itemize}
	\item Other details about the NSW program:
		\begin{itemize}
		\item \underline{Wages}:  NSW offered the trainees lower wage rates than they would've received on a regular job, but allowed their earnings to increase for satisfactory performance and attendance
		\item \underline{Post-treatment}: after their term expired, they were forced to find regular employment
		\item \underline{Job types}:  varied within sites -- gas station attendant, working at a printer shop -- and males and females were frequently performing different kinds of work
		\end{itemize}
\end{itemize}

\end{frame}
	
\begin{frame}{NSW Data}
	
	\begin{itemize}
	\item \underline{NSW data collection}:
		\begin{itemize}
		\item MDRC collected earnings and demographic information from both treatment and control at baseline and every 9 months thereafter
		\item Conducted up to 4 post-baseline interviews
		\item Different sample sizes from study to study can be confusing, but has simple explanations
		\end{itemize}
	\end{itemize}
\end{frame}
	

\begin{frame}{NSW Data}

\begin{itemize}
	\item \underline{Estimation}:
		\begin{itemize}
		\item NSW was a randomized job trainings program; therefore estimating the average treatment effect is straightforward:
			\begin{eqnarray*}
			SDO = \frac{1}{N_t}\sum_{D_i=1}Y_i - \frac{1}{N_c}\sum_{D_i=0}Y_i \approx E[Y^1-Y^0] 
			\end{eqnarray*}in large samples assuming treatment selection is independent of potential outcomes (randomization) -- i.e., $(Y^0,Y^1)\independent{D}$. 
		\end{itemize}
	\item \underline{NSW worked}: Treatment group participants' real earnings post-treatment (1978) was positive and economically meaningful -- $\approx$ \$900 (LaLonde 1986) to \$1,800 (Dehejia and Wahba 2002) depending on the sample used
\end{itemize}

\end{frame}
	
\imageframe{./lecture_includes/lalonde_table5a.png}


\begin{frame}{Switching out the control group}

\begin{itemize}
\item Think of \$800 to \$900 as the ``ground truth'' since row 1 was using the RCT 
\item LaLonde ``drops'' the experimental controls (which satisfied independence) and ``replaces'' it with six different draws from two nationally representative surveys (PSID and CPS)
\item Now the dataset contains a negatively selected treatment group compared to a nationally representative control group
\item Will selection on observable methods ``work''?
\end{itemize}

\end{frame}


\imageframe{./lecture_includes/lalonde_table5b.png}

\begin{frame}[plain,shrink=10]{Imbalanced covariates for experimental and non-experimental samples}

    \begin{center}
		\begin{table}
		\begin{tabular}{lcccccc}
		\hline \hline
		\multicolumn{3}{c}{}&
		\multicolumn{1}{c}{CPS}&
		\multicolumn{1}{c}{NSW}\\
		
		\multicolumn{1}{c}{}&
		\multicolumn{2}{c}{All} &
		\multicolumn{1}{c}{Controls} &
		\multicolumn{1}{c}{Trainees} \\

		\multicolumn{3}{c}{}&
		\multicolumn{1}{c}{$N_c=15,992$}&
		\multicolumn{1}{c}{$N_t=297$}&
		\multicolumn{1}{c}{}&
		\multicolumn{1}{c}{}\\

		\multicolumn{1}{l}{covariate}&
		\multicolumn{1}{c}{mean}&
		\multicolumn{1}{c}{(s.d.)}&
		\multicolumn{1}{c}{mean}&
		\multicolumn{1}{c}{mean}&
		\multicolumn{1}{c}{t-stat}&
		\multicolumn{1}{c}{diff}\\
		\hline
Black    & 0.09 & 0.28 & 0.07 & 0.80 & 47.04 & -0.73\\
Hispanic & 0.07 & 0.26 & 0.07 & 0.94 & 1.47 & -0.02\\
Age & 33.07 & 11.04 & 33.2 & 24.63 & 13.37  & 8.6\\
Married & 0.70 & 0.46 & 0.71 & 0.17 & 20.54 & 0.54\\
No degree & 0.30 & 0.46 & 0.30 & 0.73 & 16.27 & -0.43\\
Education & 12.0 & 2.86 & 12.03 & 10.38 & 9.85 & 1.65 \\
1975 Earnings   & 13.51 & 9.31 & 13.65 & 3.1 & 19.63 & 10.6\\
1975 Unemp  & 0.11 & 0.32 & 0.11 & 0.37 & 14.29 & -0.26\\
		\hline 
		\end{tabular}
		\end{table}
    \end{center}

\end{frame}


\begin{frame}{Dehija and Wahba (1999)}
	
	\begin{itemize}
	\item Dehejia and Wahba (DW) update LaLonde's original study using propensity score matching
		\begin{enumerate}
		\item Dehejia, Rajeev H. and Sadek Wahba (1999). 	``Causal Effects in Nonexperimental Studies: Reevaluating the Evaluation of Training Programs''. \underline{Journal of the American Statistical} \underline{Association}, vol. 94(448): 1053-1062 (\myurlshort{http://business.baylor.edu/scott_cunningham/teaching/dehejia-and-wahba-1999.pdf}{pdf})
		\end{enumerate}
	\item Can propensity score matching improve over the estimators that LaLonde examined?
	\end{itemize}
\end{frame}

\begin{frame}[plain]
	
\begin{figure}
\includegraphics[scale=0.5]{./lecture_includes/dw_1.pdf}
\end{figure}

\end{frame}

\imageframe{./lecture_includes/dw_2.pdf}

\begin{frame}{Covariate imbalance}
	

	\begin{itemize}
	\item Conditional on the propensity score, the covariates are independent of the treatment, suggesting that the distribution of covariate values should be the same for both treatment and control groups
	\item This can be checked as we have data on all three once we've estimated the propensity score
	\item DW note that the two samples have \emph{severe} imbalance on \emph{observables} -- a huge number of non-experimental controls have propensity scores almost exactly equal to 0
	\item Their analysis will ``trim'' (which will ultimately have implications for interpretation)
	\end{itemize}

\end{frame}



\imageframe{./lecture_includes/dw_3.pdf}
\imageframe{./lecture_includes/dw_4.pdf}
\imageframe{./lecture_includes/dw_5.pdf}
\imageframe{./lecture_includes/dw_6.pdf}



\clearpage
\newpage

\begin{frame}{Replies by econometricians to DW}

\begin{itemize}
\item Heckman, Smith and Todd concluded from their own work that in order for matching estimators to have low bias, you need the following:
	\begin{enumerate}
	\item A rich set of variables related to program participation and predictive of $Y^0$ labor market outcomes, 
	\item Nonexperimental comparison group be drawn from the same local labor markets as the participants and 
	\item Dependent variable (e.g., earnings) be measured in the same way for participants and nonparticipants
	\end{enumerate}
\item All three of these conditions fail to hold in DW (1999, 2002) according to Smith and Todd (2005)
\item DW also note the importance of conditioning on pre-treatment lagged outcomes (e.g., real earnings in $t-1$, $t-2$, etc.) as well as \emph{trimming}
\end{itemize}

\end{frame}


\begin{frame}{Smith and Todd, diff-in-diff, doubly robust}

\begin{itemize}
\item Difference-in-differences with propensity scores tended to work well in Smith and Todd (2005) though the effect sizes are much larger
\item In my Causal Inference II workshop, we use Sant'anna And Zhao's double robust DiD and get nearly the exact same parameter estimate as the experimental finding

\end{itemize}

\end{frame}

\begin{frame}{Coding together}

\begin{itemize}

\item Let's spend some time together trying to replicate the Lalonde study ourselves
\item I use replicate lightly -- our goal is syntax only
\item It's in the Lalonde lab on githbub for this workshop

\end{itemize}

\end{frame}


\subsection{Coarsened exact matching}



\begin{frame}{Coarsened exact matching}
	
\begin{itemize}
\item There are two kinds of matching as we've said
	\begin{enumerate}
	\item \emph{Exact matching} matches a treated unit to all of the control units with the same covariate value. Sometimes this is impossible (e.g., continuous covariate). 
	\item \emph{Approximate matching} specifies a metric to find control units that are close to the treated unit. Requires a distance metric, such as Euclidean, Mahalanobis, or the propensity score.  All of which can be implemented in Stata's \texttt{teffects}.
	\end{enumerate}
\item 	Iacus, King and Porro (2011) propose another version of matching they call coarsened exact matching (CEM). Some big picture ideas
\end{itemize}
\end{frame}

\begin{frame}{Checking imbalance}

\begin{itemize}
\item Iacus, King and Porro (2008) say that in practice approximate matching requires setting the matching solution beforehand, then checking for imbalance after.  
\item Start over, repeat, until the user is exhausted by checking for imbalance.
\end{itemize}

\end{frame}

\begin{frame}{CEM Algorithm}
	
\begin{enumerate}
\item Begin with covariates $X$. Make a copy called $X*$
\item Coarsen $X*$ according to user-defined cutpoints or CEM's automatic binning algorithm
	\begin{itemize}
	\item Schooling $\rightarrow$ less than high school, high school, some college, college, post college
	\end{itemize}
\item Create one stratum per unique observation of $X*$ and place each observation in a stratum
\item Assign these strata to the original data, $X$, and drop any observation whose stratum doesn't contain at least one treated and control unit
\end{enumerate}

You then add weights for stratum size and analyze without matching.

\end{frame}

\begin{frame}{Tradeoffs}
	
	\begin{itemize}
	\item Larger bins mean more coarsening.  This results in fewer strata.  
	\item Fewer strata result in more diverse observations within the same strata and thus higher imbalance
	\item CEM prunes both treatment and control group units, which changes the parameter of interest.  Be transparent about this as you're not estimating the ATE or the ATT when you start pruning
	\end{itemize}
	
\end{frame}

\begin{frame}{Benefits}
	
	\begin{itemize}
	\item The key benefit of CEM is that it is in a class of matching methods called \emph{monotonic imbalance bounding}
	\item MIB methods bound the maximum imbalance in some feature of the empirical distributions by an ex ante decision by the user
	\item In CEM, this ex ante choice is the coarsening decision
	\item By choosing the coarsening beforehand, users can control the amount of imbalance in the matching solution
	\item It's also wicked fast.
	\end{itemize}
\end{frame}


\begin{frame}{Imbalance}
	
	\begin{itemize}
	\item There are several ways of measuring imbalance, but here we focus on the $\mathcal{L}_1(f,g)$ measure which is$$\mathcal{L}_1(f,g) = \frac{1}{2} \sum_{l_1 \dots l_k} | f_{l_1 \dots l_k} - g_{l_1 \dots l_k} |$$where the $f$ and $g$ record the relative frequencies for the treatment and control group units.
	\item Perfect global imbalance is indicated by $\mathcal{L}_1=0$.  Larger values indicate larger imbalance between the groups, with a maximum of $\mathcal{L}_1=1$. 
	\end{itemize}
\end{frame}

\begin{frame}{Stata}
	
	\begin{itemize}
	\item Download $\texttt{cem}$ from Stata:  \texttt{ssc install cem, replace}
	\item You will automatically compute the global imbalance measure, as well as several unidimensional measures of imbalance, when using $\texttt{cem}$
	\item I got a $\mathcal{L}_1=0.55$.  What does it mean?
		\begin{itemize}
		\item By itself, it's meaningless.  It's a reference point between matching solutions.
		\item Once we have a matching solution, we will compare its $\mathcal{L}_1$ to 0.55 and gauge the increase in balance due to the matching solution from that difference.
		\item Thus $\mathcal{L}_1$ works for imbalance as $R^2$ works for model fit: the absolute values mean less than comparisons between matching solutions.
		\end{itemize}
	\end{itemize}
	
\end{frame}

\begin{frame}{More Stata}
	
	\begin{itemize}
	\item Because $\texttt{cem}$ bounds the imbalance ex ante, the most important information in the Stata output is the number of observations matched.
	\item You can also choose the coarsening as opposed to relying on the algorithm's automated binning.  
	\item Once you have estimated the strata, you regress the outcome onto the treatment and then weight the regression by $\texttt{cem_weights}$.  For instance, $$\texttt{regress re78 treat [iweight=cem\_weight]}$$
	\item For more on this, see Blackwell, et al. Stata journal article from 2009.  
	\end{itemize}
\end{frame}





\section{Concluding remarks}

\begin{frame}{Comments}

\begin{itemize}

\item Unconfoundedness means that the confounders are known and quantified, meaning they're in your data and well measured
\item Also means that within the dimensions of those covariates is an RCT (``independence'')
\item Common support is also needed with matching, but for OLS you rely on extrapolation and functional forms
\item Without a prior behavioral model guiding you, it's very hard to defend unconfoundedness (identification by convenience) 

\end{itemize}

\end{frame}

\begin{frame}{When not to use unconfoundedness methods}

\begin{itemize}
\item Individual sorting based on rationality is strong and subtle
	\begin{itemize}
	\item May be easier to defend in some situations though -- we are studying the effect of a prison assignment where if you get a suicide risk score, a team of trained inmates go meet you
	\item Only happened in 16 prisons -- I have 84 others and I know each inmate score
	\item They are picking their score to a degree, by picking their suicidality, but you wouldn't say it was done \emph{rationally}
	\end{itemize}
\item College attendance, major, marriage, children, divorce -- very hard to imagine that for people with identical covariate values they all flipped coins
\item Unconfoundedness is a strong assumption, and the weaker ones (like with respect to $Y^0$) may be easier to defend which gets you to the ATT

\end{itemize}

\end{frame}  

\begin{frame}{Common Support}

\begin{itemize}
\item Unconfoundedness says on average, $E[Y^0|D=1,X]=E[Y^0|X]$ -- that is, it doesn't depend on $D$ so you can just switch the known for unknown ones
\item But even if unconfoundedness holds doesn't mean your dataset is large enough that the one to one matches can happen -- that's failure to have enough matches because the dimensions are too large
\item So much of the literature is how to handle failed common support
\item Bias adjustment methods are one way to address that but even they can't work miracles
\end{itemize}

\end{frame}


\begin{frame}{Extrapolation and functional forms}

\begin{itemize}

\item If you do not have weak common support, then you can only estimate ATT using \emph{extrapolation} which is model dependent on functional forms
\item If you have heterogenous treatment effects, then the standard models with additive covariates is misspecified even if those are the right variables -- you'll need regression adjustment for ``saturation''
\item Same model contains ATT and ATE, so you need to decide which one you want
\item Functional form extrapolation is powerful, as it can make inference far beyond the support of the data, but it's a bit of some dark magic too
\end{itemize}

\end{frame}




\end{document}
